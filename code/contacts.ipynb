{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "# Pre-processing input data\n",
    "* Data\n",
    "    * In this task, we first load data given in Excel sheet. The data contains comments by viewers on contact lenses that they purchased. We want to tokenize the data\n",
    "    and do\n",
    "    * Sentiment Analysis\n",
    "    * Topic determination"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/bmukund/miniconda3/lib/python3.8/site-packages/spacy/util.py:837: UserWarning: [W095] Model 'en_core_web_sm' (3.2.0) was trained with spaCy v3.2 and may not be 100% compatible with the current version (3.3.0). If you see errors or degraded performance, download a newer compatible model or retrain your custom model with the current spaCy version. For more details and available updates, run: python -m spacy validate\n",
      "  warnings.warn(warn_msg)\n",
      "[nltk_data] Downloading package wordnet to /Users/bmukund/nltk_data...\n",
      "[nltk_data]   Package wordnet is already up-to-date!\n",
      "[nltk_data] Downloading package omw-1.4 to /Users/bmukund/nltk_data...\n",
      "[nltk_data]   Package omw-1.4 is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "%matplotlib inline\n",
    "%timeit\n",
    "import pandas as pd\n",
    "import string\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.feature_extraction.text import CountVectorizer, TfidfVectorizer\n",
    "import sys\n",
    "from collections import Counter\n",
    "import random\n",
    "import ast\n",
    "import re\n",
    "import scipy\n",
    "\n",
    "sys.path.append('pymodules')\n",
    "# This class contains some utility functions Word2Vec, stop words etc. etc.\n",
    "import pymodules.preprocessing_class as pc\n",
    "\n",
    "# gender gueser\n",
    "import gender_guesser.detector as gd\n",
    "\n",
    "# for dictionary method synonym finder using wordnet\n",
    "import nltk\n",
    "nltk.download('wordnet')\n",
    "nltk.download('omw-1.4')\n",
    "from nltk.corpus import wordnet as wn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "def find_wordnet_synonyms(word_list, type_of_word=None):\n",
    "    \"\"\"\n",
    "    Function to find synonyms of words given in the input respecting the type of word (noun or adjective or verb)\n",
    "    It is assumed that the word_list, the input is a list\n",
    "    @return lemmatized synonyms\n",
    "    \"\"\"\n",
    "    synonyms = set()\n",
    "    for word_to_look in word_list:\n",
    "        #print(f\"looking for synonyms of word:{word_to_look}\")\n",
    "        for syn in wn.synsets(word_to_look, pos=type_of_word):\n",
    "            for i in syn.lemmas():\n",
    "                synonyms.add(i.name())\n",
    "    #print(f\"Synonyms:\\n {synonyms}\")\n",
    "    return synonyms"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "outputs": [],
   "source": [
    "def first_name(x):\n",
    "    \"\"\"\n",
    "    Function to get the first name so that we can guess the gender\n",
    "    * We determine the first name from the given string.\n",
    "    * We also remove any digits from the name.\n",
    "    * We use Space to split names\n",
    "    \"\"\"\n",
    "    x_split = str(x).split()\n",
    "    fname = x_split[0]\n",
    "    # remove reference to digits. Now after removal, there could be some misclassification, but that is ok ..\n",
    "    fname_p = re.sub(r'[0-9]+', \"\", fname)\n",
    "    ret_str = fname_p.capitalize()\n",
    "    return ret_str\n"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "outputs": [],
   "source": [
    "def read_input(filename, sheet_name, filter_columns):\n",
    "    df = pd.read_excel(filename, sheet_name=sheet_name, index_col='REVIEW_DATE')\n",
    "    df  = df.drop(columns = filter_columns, axis=1)\n",
    "    return df"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "filename = \"data/Master-data_Q42021.xlsx\"\n",
    "sheet_name = 'Scrubbed_data'\n",
    "# We don't need these columns\n",
    "not_needed = ['OVERALL_RATING', 'COMFORT_RATING', 'VISION_RATING', 'VALUE_FOR_MONEY', 'PROS', 'CONS', 'ORIGINAL_SOURCE', 'REPLY_FROM_ACCUVUE',\n",
    "              'PRODUCT_LINK', 'WEBSITE']\n",
    "\n",
    "text_data = read_input(filename, sheet_name, not_needed)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "# Let us figure out the gender from the names and drop the names column\n",
    "# We use gender_guesser package.\n",
    "#text_data['AUTHOR'] = text_data['AUTHOR'].astype(str)\n",
    "gdx = gd.Detector()\n",
    "text_data['GENDER'] = text_data.AUTHOR.apply(first_name).map(lambda x: gdx.get_gender(x))\n",
    "\n",
    "# Drop the author column now\n",
    "text_data.drop(columns = ['AUTHOR'], axis=1, inplace=True)\n",
    "\n",
    "# Check the gender counts just to see how the data looks like\n",
    "text_data.GENDER.value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "# Consolidate the comments into one column\n",
    "# Comments can occur both in title and in Comment columns. \n",
    "text_data['COMMENT'] = text_data['TITLE'].astype(str).fillna(\"\") + \" \" + text_data['COMMENTS'].astype(str).fillna(\"\")\n",
    "text_data.drop(columns = ['TITLE', 'COMMENTS'], axis=1, inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "# clean rating\n",
    "# replace N = No rating with 0. We do this because rating is assumed to be numeric, not categorical\n",
    "text_data['RATING'].replace('N', '0', inplace=True)\n",
    "# convert rating to integers\n",
    "text_data['RATING'] = text_data['RATING'].apply(lambda x: int(x))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "# display results\n",
    "text_data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "### Tokenization <a class=\"anchor\" id=\"first-bullet\"></a>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "jupyter": {
     "outputs_hidden": false
    },
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "## regex for tokenization\n",
    "# Ref: http://sentiment.christopherpotts.net/code-data/happyfuntokenizing.py\n",
    "emoticon_string = r\"\"\"\n",
    "    (?:\n",
    "      [<>]?\n",
    "      [:;=8]                     # eyes\n",
    "      [\\-o\\*\\']?                 # optional nose\n",
    "      [\\)\\]\\(\\[dDpP/\\:\\}\\{@\\|\\\\] # mouth\n",
    "      |\n",
    "      [\\)\\]\\(\\[dDpP/\\:\\}\\{@\\|\\\\] # mouth\n",
    "      [\\-o\\*\\']?                 # optional nose\n",
    "      [:;=8]                     # eyes\n",
    "      [<>]?\n",
    "    )\"\"\"\n",
    "\n",
    "# The components of the tokenizer:\n",
    "regex_strings = (\n",
    "    # Phone numbers:\n",
    "    r\"\"\"\n",
    "    (?:\n",
    "      (?:            # (international)\n",
    "        \\+?[01]\n",
    "        [\\-\\s.]*\n",
    "      )?\n",
    "      (?:            # (area code)\n",
    "        [\\(]?\n",
    "        \\d{3}\n",
    "        [\\-\\s.\\)]*\n",
    "      )?\n",
    "      \\d{3}          # exchange\n",
    "      [\\-\\s.]*\n",
    "      \\d{4}          # base\n",
    "    )\"\"\"\n",
    "    ,\n",
    "    # Emoticons:\n",
    "    emoticon_string\n",
    "    ,\n",
    "    # HTML tags:\n",
    "    r\"\"\"<[^>]+>\"\"\"\n",
    "    ,\n",
    "    # Twitter username:\n",
    "    r\"\"\"(?:@[\\w_]+)\"\"\"\n",
    "    ,\n",
    "    # Twitter hashtags:\n",
    "    r\"\"\"(?:\\#+[\\w_]+[\\w\\'_\\-]*[\\w_]+)\"\"\"\n",
    "    ,\n",
    "    # Remaining word types:\n",
    "    r\"\"\"\n",
    "    (?:[a-z][a-z'\\-_]+[a-z])       # Words with apostrophes or dashes.\n",
    "    |\n",
    "    (?:[+\\-]?\\d+[,/.:-]\\d+[+\\-]?)  # Numbers, including fractions, decimals.\n",
    "    |\n",
    "    (?:[\\w_]+)                     # Words without apostrophes or dashes.\n",
    "    |\n",
    "    (?:\\.(?:\\s*\\.){1,})            # Ellipsis dots.\n",
    "    |\n",
    "    (?:\\S)                         # Everything else that isn't whitespace.\n",
    "    \"\"\",\n",
    "    r\"\"\"\n",
    "    (?x)                # set flag to allow verbose regexps (to separate logical sections of pattern and add comments)\n",
    "    \\w+(?:-\\w+)*        # preserve expressions with internal hyphens as single tokens\n",
    "    | [][.,;\"'?():-_`]  # preserve punctuation as separate tokens\n",
    "    \"\"\"\n",
    ")\n",
    "word_re = re.compile(pattern=r\"\"\"(%s)\"\"\" % \"|\".join(regex_strings), flags=re.VERBOSE | re.I)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "jupyter": {
     "outputs_hidden": false
    },
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "comments_data = text_data.COMMENT\n",
    "prep_comments = pc.RawDocs(comments_data,  # series of documents\n",
    "                  lower_case=True,  # whether to lowercase the text in the firs cleaning step\n",
    "                  stopwords='long',  # type of stopwords to initialize\n",
    "                  contraction_split=True,  # wheter to split contractions or not\n",
    "                  tokenization_pattern=word_re  # custom tokenization patter\n",
    "                  )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "jupyter": {
     "outputs_hidden": false
    },
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "# notice that the documents from the object are identical to the ones from the pandas series\n",
    "#comments_data\n",
    "i = 0\n",
    "print(\"Document from the pandas series:\\n\", comments_data[i])\n",
    "print(\"\\n-------------------------\\n\")\n",
    "print(\"Document from preprocessing object:\\n\", prep_comments.docs[i])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "jupyter": {
     "outputs_hidden": false
    },
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "# lower-case text, expand contractions and initialize stopwords list\n",
    "prep_comments.basic_cleaning()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "# explore an example after the basic cleaning has been applied\n",
    "i = 0\n",
    "print(comments_data[i])\n",
    "print()\n",
    "print(prep_comments.docs[i])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "# now we can split the documents into tokens\n",
    "prep_comments.tokenize_text()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "i = 0\n",
    "print(comments_data[i])\n",
    "print()\n",
    "print(prep_comments.tokens[i])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "punctuation = string.punctuation\n",
    "punctuation = punctuation.replace(\"-\", \"\") # remove the hyphen from the punctuation string\n",
    "punctuation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "prep_comments.token_clean(length=2,                 # remove tokens with less than this number of characters\n",
    "                 punctuation=punctuation,           # remove custom list of punctuation characters\n",
    "                 numbers = True                     # remove numbers\n",
    "                 )\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "i = 0\n",
    "print(comments_data[i])\n",
    "print()\n",
    "print(prep_comments.tokens[i])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "# get the list of stopwords provided earlier\n",
    "print(sorted(prep_comments.stopwords))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "# we need to specificy that we want to remove the stopwords from the \"tokens\"\n",
    "prep_comments.stopword_remove('tokens')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "i = 0\n",
    "print(comments_data[i])\n",
    "print()\n",
    "print(prep_comments.tokens[i])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "# stemming\n",
    "# pre_comments.stem()\n",
    "\n",
    "# apply lemmatization to all documents (takes a very long time so we will avoid it for now)\n",
    "prep_comments.lemmatize()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "# compare all versions of the same raw sentences\n",
    "i = 0\n",
    "print(comments_data[i])\n",
    "print()\n",
    "print(prep_comments.tokens[i])\n",
    "print()\n",
    "# print(prep_comments.stems[i])\n",
    "# print()\n",
    "print(prep_comments.lemmas[i])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "prep_comments.get_term_ranking(items='tokens', score_type='df')\n",
    "prep_comments.df_ranking['tokens'][:15]\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "TF-IDF weights more words that occur frequently but in less number of documents. This seems to skew ranking towards advertisement like reviews. (See output) It is not clear if such counting is relevant for this project."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "prep_comments.get_term_ranking(items='tokens', score_type='tfidf')\n",
    "prep_comments.tfidf_ranking['tokens'][:10]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "prep_comments.df_ranking['tokens'][-10:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "prep_comments.tfidf_ranking['tokens'][-15:]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "plt.figure(figsize=(10,6))\n",
    "plt.plot([x[0] for x in prep_comments.df_ranking['tokens']])\n",
    "plt.title('Document frequency ranking')\n",
    "plt.ylabel(\"Document frequency\")\n",
    "plt.xlabel(\"Term ranking\")\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "# we can use a log-log scale to observe more clearly the power-law distribution (Zipf's law)\n",
    "plt.figure(figsize=(10,6))\n",
    "plt.loglog([x[0] for x in prep_comments.df_ranking['tokens']])\n",
    "plt.title('Document frequency ranking (log-log)')\n",
    "plt.ylabel(\"log document frequency\")\n",
    "plt.xlabel(\"log term ranking\")\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "plt.figure(figsize=(10,6))\n",
    "plt.plot([x[0] for x in prep_comments.tfidf_ranking['tokens']])\n",
    "plt.title('Tf-idf ranking')\n",
    "plt.ylabel(\"tf-idf\")\n",
    "plt.xlabel(\"Term ranking\")\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "### Vectorization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "# simple auxiliary function to override the preprocessing done by sklearn\n",
    "def do_nothing(doc):\n",
    "    return doc\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "# create a CountVectorizer object using our preprocessed text\n",
    "count_vectorizer = CountVectorizer(encoding='utf-8',\n",
    "                                   preprocessor=do_nothing,  # apply no additional preprocessing\n",
    "                                   tokenizer=do_nothing,     # apply no additional tokenization\n",
    "                                   lowercase=False,\n",
    "                                   strip_accents=None,\n",
    "                                   stop_words=None,\n",
    "                                   ngram_range=(1, 1),       # generate only unigrams\n",
    "                                   analyzer='word',          # analysis at the word-level\n",
    "                                   max_df=0.5,              # ignore tokens that have a higher document frequency (can be int or percent)\n",
    "                                   min_df=500,                # ignore tokens that have a lowe document frequency (can be int or percent)\n",
    "                                   max_features=None,        # we could impose a maximum number of vocabulary terms\n",
    "                                   )\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "# transform our preprocessed tokens into a document-term matrix\n",
    "dt_matrix = count_vectorizer.fit_transform(prep_comments.tokens)\n",
    "print(f\"Document-term matrix created with shape: {dt_matrix.shape}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "# we can access a dictionary that maps between words and positions of the document-term matrix\n",
    "# list(count_vectorizer.vocabulary_.items())[0:10]\n",
    "id_word_indexer = pd.DataFrame(count_vectorizer.vocabulary_.items())\n",
    "id_word_indexer"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "### Dictionary methods\n",
    "Dictionary methods ai to find synonyms of words and add to the corpus. In our case we want to classify our corpus based on these four terms by identifying synonyms from the text\n",
    "* Identify \"product\"\n",
    "* Identify \"service\"\n",
    "* Identify \"quality\"\n",
    "* Identify \"price\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "### Finding synonyms using WordNet for the words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "product_words = ['gadget', 'contraption', 'appliance', 'widget', 'equipment', 'contrivance', 'gizmo', 'product', 'merchandise', 'ware', 'gismo']\n",
    "service_words = ['service', 'assist', 'help', 'aid']\n",
    "quality_words = ['quality', 'built', 'refurbish', 'comfort', 'relief']\n",
    "price_words = ['price', 'money', 'cash', 'cheap', 'costly', 'pricey', 'discount', 'payment', 'rebate', 'cost']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "#### Now we use the synonym finder to generate extra set of words for identifying classification words.\n",
    "We then use the resulting set of words as tokens to be found in our corpus. Because the synonym words from the wordnet synonym finder may not be entirely suitable to be used automatically\n",
    "we filter the result. Particularly, some words could be used in different senses (noun, adjective etc.) and could determine different classification. Given that we know the domain here,\n",
    "we want to make sure that the synonyms we find don't add to the ambiguity\n",
    "For example, one of the synonyms for \"ware\" is \"convenience\" (as in mode of convenience). But this could also mean \"ease of use\", which would come under \"quality\" and not under \"product\"\n",
    "In order to avoid this, we manually filter out the output synonyms of wordnet results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "syns_indicating_product = find_wordnet_synonyms(product_words, wn.NOUN)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "syns_indicating_service = find_wordnet_synonyms(service_words, wn.NOUN)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "syns_indicating_quality = find_wordnet_synonyms(quality_words, wn.NOUN)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "syns_indicating_price = find_wordnet_synonyms(price_words, wn.NOUN)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "tokens_indicating_product = product_words\n",
    "tokens_indicating_service = service_words\n",
    "tokens_indicating_quality = quality_words\n",
    "tokens_indicating_price = price_words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "# vocabulary's key is the feature word and the value is the feature-word's index in the feature column ...\n",
    "service_indicator_token_ids = [v for k,v in count_vectorizer.vocabulary_.items() if k in tokens_indicating_service]\n",
    "print(f\"{len(service_indicator_token_ids)} tokens found in vocabulary indicating service, {service_indicator_token_ids}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "product_indicator_token_ids = [v for k,v in count_vectorizer.vocabulary_.items() if k in tokens_indicating_product]\n",
    "print(f\"{len(product_indicator_token_ids)} tokens found in vocabulary indicating product, {product_indicator_token_ids}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "quality_indicator_token_ids = [v for k,v in count_vectorizer.vocabulary_.items() if k in tokens_indicating_quality]\n",
    "print(f\"{len(quality_indicator_token_ids)} tokens found in vocabulary indicating quality, {quality_indicator_token_ids}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "price_indicator_token_ids = [v for k,v in count_vectorizer.vocabulary_.items() if k in tokens_indicating_price]\n",
    "print(f\"{len(price_indicator_token_ids)} tokens found in vocabulary indicating price, {price_indicator_token_ids}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "service_indicator_counts = dt_matrix.tocsr()[:, service_indicator_token_ids]\n",
    "# for a given data, count all such tokens that indicate service and presumably, one can add this as a new column to the data itself\n",
    "service_indicator_counts = service_indicator_counts.sum(axis=1)\n",
    "service_indicator_counts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "product_indicator_counts = dt_matrix.tocsr()[:, product_indicator_token_ids]\n",
    "# for a given data, count all such tokens that indicate product and presumably, one can add this as a new column to the data itself\n",
    "product_indicator_counts = product_indicator_counts.sum(axis=1)\n",
    "np.array(product_indicator_counts).ravel()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "price_indicator_counts = dt_matrix.tocsr()[:, price_indicator_token_ids]\n",
    "# for a given data, count all such tokens that indicate price and presumably, one can add this as a new column to the data itself\n",
    "price_indicator_counts = price_indicator_counts.sum(axis=1)\n",
    "np.array(price_indicator_counts).ravel()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "quality_indicator_counts = dt_matrix.tocsr()[:, quality_indicator_token_ids]\n",
    "# for a given data, count all such tokens that indicate quality and presumably, one can add this as a new column to the data itself\n",
    "quality_indicator_counts = quality_indicator_counts.sum(axis=1)\n",
    "np.array(quality_indicator_counts).ravel()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "# index 14 - service, 15-product, 16-quality, 17-price\n",
    "for idx in [14, 15, 16, 17]:\n",
    "    unique, counts = np.unique(dt_matrix.toarray()[:, idx], return_counts=True)\n",
    "    print(f\"Index:{idx}, value:\\n{np.asarray((unique, counts)).T}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "text_data['service'] = np.array(service_indicator_counts).ravel()\n",
    "text_data['product'] = np.array(product_indicator_counts).ravel()\n",
    "text_data['quality'] = np.array(quality_indicator_counts).ravel()\n",
    "text_data['price'] = np.array(price_indicator_counts).ravel()\n",
    "text_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "text_data['RATING'].replace('N', '-1', inplace=True)\n",
    "text_data.RATING.value_counts()\n",
    "text_data['RATING'] = text_data['RATING'].apply(lambda x: int(x))\n",
    "\n",
    "data_agg = text_data.groupby(['price'], as_index=False).agg({'COMMENT': 'sum', 'RATING':'mean'})\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "data_agg"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "data_agg = text_data.groupby(['service'], as_index=False).agg({'COMMENT': 'sum', 'RATING':'mean'})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "data_agg"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "## FOR SVM, we need to make a matrix with proper column names\n",
    "# Also, we need another column that denotes the review\n",
    "# We also need to normalize the data\n",
    "df_svm = pd.DataFrame(dt_matrix.toarray())\n",
    "df_svm.rename(columns=id_word_indexer.to_dict()[0], inplace=True)\n",
    "df_svm"
   ]
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "7478654ab1aa58977e1af457aae5317775386ee06614bd651985e9aec83741b6"
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}