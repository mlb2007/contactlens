{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "# Pre-processing input data\n",
    "* Data\n",
    "    * In this task, we first load data given in Excel sheet. The data contains comments by viewers on contact lenses that they purchased. We want to tokenize the data\n",
    "    and do\n",
    "    * Sentiment Analysis\n",
    "    * Topic determination"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "* We download nltk wordnet which provides us with a vocabulary of words, that we use to build our dictionary\n",
    "* We also guess the gender using gender guesser package"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "%timeit\n",
    "import pandas as pd\n",
    "import string\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.feature_extraction.text import CountVectorizer, TfidfVectorizer\n",
    "import sys\n",
    "from collections import Counter\n",
    "import random\n",
    "import ast\n",
    "import re\n",
    "import scipy\n",
    "\n",
    "# category\n",
    "from pandas.api.types import CategoricalDtype\n",
    "\n",
    "sys.path.append('pymodules')\n",
    "# This class contains some utility functions Word2Vec, stop words etc. etc.\n",
    "import pymodules.preprocessing_class as pc\n",
    "\n",
    "# gender gueser\n",
    "import gender_guesser.detector as gd\n",
    "\n",
    "# for dictionary method synonym finder using wordnet\n",
    "import nltk\n",
    "try:\n",
    "    nltk.data.find('corpora/wordnet')\n",
    "except LookupError:\n",
    "    nltk.download('wordnet')\n",
    "try:\n",
    "    nltk.data.find('corpora/omw-1.4')\n",
    "except LookupError:\n",
    "    nltk.download('omw-1.4')\n",
    "\n",
    "from nltk.corpus import wordnet as wn"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "### Find synonyms to add to the vocabulary of words.\n",
    "This is required for sentiment analysis so that most of the comment words by users are captured"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "def find_wordnet_synonyms(word_list, type_of_word=None):\n",
    "    \"\"\"\n",
    "    Function to find synonyms of words given in the input respecting the type of word (noun or adjective or verb)\n",
    "    It is assumed that the word_list, the input is a list\n",
    "    @return lemmatized synonyms\n",
    "    \"\"\"\n",
    "    synonyms = set()\n",
    "    for word_to_look in word_list:\n",
    "        #print(f\"looking for synonyms of word:{word_to_look}\")\n",
    "        for syn in wn.synsets(word_to_look, pos=type_of_word):\n",
    "            for i in syn.lemmas():\n",
    "                synonyms.add(i.name())\n",
    "    #print(f\"Synonyms:\\n {synonyms}\")\n",
    "    return synonyms"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "### Extract first name\n",
    "* Used in guessing gender"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "def first_name(x):\n",
    "    \"\"\"\n",
    "    Function to get the first name so that we can guess the gender\n",
    "    * We determine the first name from the given string.\n",
    "    * We also remove any digits from the name.\n",
    "    * We use Space to split names\n",
    "    \"\"\"\n",
    "    x_split = str(x).split()\n",
    "    fname = x_split[0]\n",
    "    # remove reference to digits. Now after removal, there could be some misclassification, but that is ok ..\n",
    "    fname_p = re.sub(r'[0-9]+', \"\", fname)\n",
    "    ret_str = fname_p.capitalize()\n",
    "    return ret_str\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "### Excel File reader\n",
    "* The date is used as index\n",
    "* Depending on time of the year, comments may vary. So we can group data by dates and do analysis if required"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "def read_input(filename, sheet_name, filter_columns):\n",
    "    df = pd.read_excel(filename, sheet_name=sheet_name, index_col='REVIEW_DATE')\n",
    "    print(f\"Input columns:{df.columns}\")\n",
    "    df  = df.drop(columns = filter_columns, axis=1)\n",
    "    return df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "## Start EDA analysis of input data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "filename = \"data/Master-data_Q42021.xlsx\"\n",
    "sheet_name = 'Scrubbed_data'\n",
    "# We don't need these columns\n",
    "not_needed = ['OVERALL_RATING', 'COMFORT_RATING', 'VISION_RATING', 'VALUE_FOR_MONEY', 'PROS', 'CONS', 'ORIGINAL_SOURCE', 'REPLY_FROM_ACCUVUE',\n",
    "              'PRODUCT_LINK', 'WEBSITE']\n",
    "\n",
    "text_data = read_input(filename, sheet_name, not_needed)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "## Figure out the gender distribution\n",
    "* It is interesting to note that majority of comments are by female, twice as much femals have commented compared to males"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "# Let us figure out the gender from the names and drop the names column\n",
    "# We use gender_guesser package.\n",
    "#text_data['AUTHOR'] = text_data['AUTHOR'].astype(str)\n",
    "gdx = gd.Detector()\n",
    "text_data['GENDER'] = text_data.AUTHOR.apply(first_name).map(lambda x: gdx.get_gender(x))\n",
    "\n",
    "# Drop the author column now\n",
    "text_data.drop(columns = ['AUTHOR'], axis=1, inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "# Check the gender counts just to see how the data looks like\n",
    "text_data.GENDER.value_counts().plot(kind='bar')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "## Extract comments of the users\n",
    "* Comments are found in two columns 'COMMENTS' column and 'TITLE' column.\n",
    "    * This is because when the user inputs his/her reviews, sometimes they put their comments in the title itself and not in the box for comments. We need to take care of that too.\n",
    "* Consolidate the comments into one column called 'COMMENT'.\n",
    "* Comments can occur both in title and in Comment columns."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "# Consolidate the comments into one column\n",
    "# Comments can occur both in title and in Comment columns. \n",
    "text_data['COMMENT'] = text_data['TITLE'].astype(str).fillna(\"\") + \" \" + text_data['COMMENTS'].astype(str).fillna(\"\")\n",
    "text_data.drop(columns = ['TITLE', 'COMMENTS'], axis=1, inplace=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "## Find user review rating\n",
    "* This is our response variable, the output variable\n",
    "* Ratings are curated so that they are all integers.\n",
    "* Ratings vary from [0-5]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "# clean rating\n",
    "# replace N = No rating with 0. We do this because rating is assumed to be numeric, not categorical\n",
    "text_data['RATING'].replace('N', '0', inplace=True)\n",
    "# convert rating to integers\n",
    "text_data['RATING'] = text_data['RATING'].apply(lambda x: int(x))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "### Plot rating"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "text_data['RATING']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "text_data['RATING'].plot(kind='density')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "import seaborn as sns\n",
    "sns.displot(text_data, x=\"RATING\")\n",
    "## complicated plot ...\n",
    "#ax = sns.countplot(x=text_data['RATING'], order=text_data['RATING'].value_counts(ascending=False).index)\n",
    "#abs_values = text_data['RATING'].value_counts(ascending=False).values\n",
    "#ax.bar_label(container=ax.containers[0], labels=abs_values)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Sentiment from User Ratings using threshold:\n",
    "* Rating Values < threshold => Negative sentiment\n",
    "* Rating Values = threshold => Neutral sentiment\n",
    "* Rating Values > threshold => Positive sentiment"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "sentiment_threshold = 3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "# attach sentiment to ratings\n",
    "def find_sentiment(senti_threshold):\n",
    "    def _find_sentiment(rating):\n",
    "        choices = [0, 1, 2]\n",
    "        conditions = [rating < senti_threshold, rating == senti_threshold, rating > senti_threshold]\n",
    "        senti = np.select(conditions, choices)\n",
    "        return senti\n",
    "    return _find_sentiment\n",
    "\n",
    "SENTIMENT_SERIES = df['RATING'].apply(find_sentiment(sentiment_threshold)).astype('category')\n",
    "df_svm['SENTI'] = SENTIMENT_SERIES.values\n",
    "df_svm['SENTI'] = df_svm['SENT'].astype('category')\n",
    "sentiments = df_svm._SENTIMENT_.value_counts()\n",
    "print(sentiments)\n",
    "\n",
    "import seaborn as sns\n",
    "ax = sns.countplot(x=df_svm['_SENTIMENT_'], order=df_svm['_SENTIMENT_'].value_counts(ascending=False).index)\n",
    "abs_values = df_svm['_SENTIMENT_'].value_counts(ascending=False).values\n",
    "ax.bar_label(container=ax.containers[0], labels=abs_values)\n",
    "ax.set(xticklabels=['>3', '<3', '=3'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "### Check output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "# display results\n",
    "text_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "### Tokenization <a class=\"anchor\" id=\"first-bullet\"></a>\n",
    "* Based on tokenization http://sentiment.christopherpotts.net/code-data/happyfuntokenizing.py\n",
    "* A tokenizer is a function that splits a string of text into words based on some delimiter that separates the strings.\n",
    "* In this usae case, we use a regular expression to determine what a word means. Typically comments ca ne equated to tweets\n",
    "  in its brevity and also for the language used. Hence we use the same regular expressions as we would for parsing a tweet."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "jupyter": {
     "outputs_hidden": false
    },
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "## regex for tokenization\n",
    "# Ref: http://sentiment.christopherpotts.net/code-data/happyfuntokenizing.py\n",
    "emoticon_string = r\"\"\"\n",
    "    (?:\n",
    "      [<>]?\n",
    "      [:;=8]                     # eyes\n",
    "      [\\-o\\*\\']?                 # optional nose\n",
    "      [\\)\\]\\(\\[dDpP/\\:\\}\\{@\\|\\\\] # mouth\n",
    "      |\n",
    "      [\\)\\]\\(\\[dDpP/\\:\\}\\{@\\|\\\\] # mouth\n",
    "      [\\-o\\*\\']?                 # optional nose\n",
    "      [:;=8]                     # eyes\n",
    "      [<>]?\n",
    "    )\"\"\"\n",
    "\n",
    "# The components of the tokenizer:\n",
    "regex_strings = (\n",
    "    # Phone numbers:\n",
    "    r\"\"\"\n",
    "    (?:\n",
    "      (?:            # (international)\n",
    "        \\+?[01]\n",
    "        [\\-\\s.]*\n",
    "      )?\n",
    "      (?:            # (area code)\n",
    "        [\\(]?\n",
    "        \\d{3}\n",
    "        [\\-\\s.\\)]*\n",
    "      )?\n",
    "      \\d{3}          # exchange\n",
    "      [\\-\\s.]*\n",
    "      \\d{4}          # base\n",
    "    )\"\"\"\n",
    "    ,\n",
    "    # Emoticons:\n",
    "    emoticon_string\n",
    "    ,\n",
    "    # HTML tags:\n",
    "    r\"\"\"<[^>]+>\"\"\"\n",
    "    ,\n",
    "    # Twitter username:\n",
    "    r\"\"\"(?:@[\\w_]+)\"\"\"\n",
    "    ,\n",
    "    # Twitter hashtags:\n",
    "    r\"\"\"(?:\\#+[\\w_]+[\\w\\'_\\-]*[\\w_]+)\"\"\"\n",
    "    ,\n",
    "    # Remaining word types:\n",
    "    r\"\"\"\n",
    "    (?:[a-z][a-z'\\-_]+[a-z])       # Words with apostrophes or dashes.\n",
    "    |\n",
    "    (?:[+\\-]?\\d+[,/.:-]\\d+[+\\-]?)  # Numbers, including fractions, decimals.\n",
    "    |\n",
    "    (?:[\\w_]+)                     # Words without apostrophes or dashes.\n",
    "    |\n",
    "    (?:\\.(?:\\s*\\.){1,})            # Ellipsis dots.\n",
    "    |\n",
    "    (?:\\S)                         # Everything else that isn't whitespace.\n",
    "    \"\"\",\n",
    "    r\"\"\"\n",
    "    (?x)                # set flag to allow verbose regexps (to separate logical sections of pattern and add comments)\n",
    "    \\w+(?:-\\w+)*        # preserve expressions with internal hyphens as single tokens\n",
    "    | [][.,;\"'?():-_`]  # preserve punctuation as separate tokens\n",
    "    \"\"\"\n",
    ")\n",
    "word_re = re.compile(pattern=r\"\"\"(%s)\"\"\" % \"|\".join(regex_strings), flags=re.VERBOSE | re.I)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "### Tokenize using regex\n",
    "* We take the data consolidated in 'COMMENT' column of dataframe and tokenize it as follows\n",
    "* We convert all text to lower case\n",
    "* We remove commonly used stop words and prepositions such as 'in', 'with'. These stopwords have been compiled for us and we simply use a database of stop words\n",
    "* We do a lazy tokenization, in that we take all the required inputs and create an object called 'RawDocs'\n",
    "    * class RawDocs has a lot of functions other than tokenization, (for example, join two consecutive words (bi-gramming), lemmatization etc.)\n",
    "* We use a list of \"short\" stop words. Further, we remove \"not\", \"no\" etc. from the stop words list because we need such words to be counted as legitimate words for sentiment analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "jupyter": {
     "outputs_hidden": false
    },
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "comments_data = text_data.COMMENT\n",
    "prep_comments = pc.RawDocs(comments_data,  # series of documents\n",
    "                  lower_case=True,  # whether to lowercase the text in the firs cleaning step\n",
    "                  stopwords='short',  # type of stopwords to initialize\n",
    "                  contraction_split=True,  # wheter to split contractions or not\n",
    "                  tokenization_pattern=word_re  # custom tokenization patter\n",
    "                  )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "* We should notice that the RawDocs object created is identical to the input data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "jupyter": {
     "outputs_hidden": false
    },
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "#comments_data\n",
    "i = 0\n",
    "print(f\"Document data at index[{i}:\\n {comments_data[i]}\")\n",
    "print(\"\\n-------------------------\\n\")\n",
    "print(f\"Document data after objectifying input with tokenization procedures added[{i} :\\n {prep_comments.docs[i]}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "### Basic cleaning\n",
    "* Remove stop words. One can input custom stop words list as argument to the function in addition to default\n",
    "    * Here we use standard stop words\n",
    "* We expand contractions like \"don't\", \"can't\" etc. to make word explicit for NLP"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "jupyter": {
     "outputs_hidden": false
    },
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "# lower-case text, expand contractions and initialize stopwords list\n",
    "prep_comments.basic_cleaning()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "### Print the comments after basic cleaning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "# explore an example after the basic cleaning has been applied\n",
    "test_index = 0\n",
    "print(f\"Data at index[{test_index}] after basic cleaning:\\n {prep_comments.docs[test_index]}\")\n",
    "test_index = -1\n",
    "print(f\"Data at index[{test_index}] after basic cleaning:\\n {prep_comments.docs[test_index]}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "### Tokenize\n",
    "* Now tokenize the data that is already cached in RawDocs object called prep_comments"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "# now we can split the documents into tokens\n",
    "prep_comments.tokenize_text()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "### print data after tokenization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "test_index = 0\n",
    "print(f\"Original comments at index[{test_index}]:\\n {comments_data[test_index]}\")\n",
    "print(f\"After tokenization, comments:\\n {prep_comments.tokens[test_index]}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "### Remove punctuations\n",
    "* remove tokens with less than TWO characters\n",
    "* remove custom list of punctuation characters\n",
    "* remove numbers\n",
    "* remove hypens"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "punctuation = string.punctuation\n",
    "punctuation = punctuation.replace(\"-\", \"\") # remove the hyphen from the punctuation string\n",
    "# punctuation\n",
    "prep_comments.token_clean(length=2,                 # remove tokens with less than this number of characters\n",
    "                 punctuation=punctuation,           # remove custom list of punctuation characters\n",
    "                 numbers = True                     # remove numbers\n",
    "                 )\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "### Check after removal of punctuations\n",
    "* Note the removal of exclamation marks."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "test_index = 22\n",
    "print(f\"Original comments at index[{test_index}]:\\n {comments_data[test_index]}\")\n",
    "print(f\"After tokenization, comments:\\n {prep_comments.tokens[test_index]}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "### Stopwords to be removed"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "# get the list of stopwords provided earlier\n",
    "print(sorted(prep_comments.stopwords))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "### Now remove stopwords\n",
    "* tokens are cached inside prep_comments object as 'tokens', i.e. prep_comments.tokens will give us back the tokenized user comment dat"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "# we need to specificy that we want to remove the stopwords from the \"tokens\"\n",
    "prep_comments.stopword_remove('tokens')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "### Check after removal of stop words\n",
    "* Note that numbers, exclamation marks have been removed, some stop words like \"that\", \"my\" has been removed"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "test_index = 42\n",
    "print(f\"Original comments at index[{test_index}]:\\n {comments_data[test_index]}\")\n",
    "print(f\"After tokenization, comments:\\n {prep_comments.tokens[test_index]}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "### Lemmatize\n",
    "From: https://nlp.stanford.edu/IR-book/html/htmledition/stemming-and-lemmatization-1.html\n",
    "\"\n",
    "The goal of both stemming and lemmatization is to reduce inflectional forms and sometimes derivationally related forms of a word to a common base form. For instance:\n",
    "\n",
    "am, are, is $\\Rightarrow$ be\n",
    "\n",
    "car, cars, car's, cars' $\\Rightarrow$ car\n",
    "\n",
    "The result of this mapping of text will be something like:\n",
    "the boy's cars are different colors $\\Rightarrow$ the boy car be differ color\n",
    "\n",
    "However, the two words differ in their flavor.\n",
    "\n",
    "Stemming usually refers to a crude heuristic process that chops off the ends of words in the hope of achieving this goal correctly most of the time, and often includes the removal of derivational affixes.\n",
    "\n",
    "Lemmatization usually refers to doing things properly with the use of a vocabulary and morphological analysis of words, normally aiming to remove inflectional endings only and to return the base or dictionary form of a word, which is known as the lemma.\n",
    "\n",
    "If confronted with the token saw, stemming might return just s, whereas lemmatization would attempt to return either see or saw depending on whether the use of the token was as a verb or a noun. The two may also differ in that stemming most commonly collapses derivationally related words, whereas lemmatization commonly only collapses the different inflectional forms of a lemma.\n",
    "\n",
    "Linguistic processing for stemming or lemmatization is often done by an additional plug-in component to the indexing process, and a number of such components exist, both commercial and open-source.\n",
    "The most common algorithm for stemming English, and one that has repeatedly been shown to be empirically very effective, is Porter's algorithm (Porter, 1980).\n",
    "\"\n",
    "\n",
    "* We do lemmatization, which is a slow process compared to stemming in our work here"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "# apply lemmatization (SLOW)\n",
    "prep_comments.lemmatize()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "### Check lemmatization\n",
    "* Note 'shipping' has been lemmatized to 'ship'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "# compare all versions of the same raw sentences\n",
    "test_index = 22\n",
    "print(f\"Original comments at index[{test_index}]:\\n {comments_data[test_index]}\")\n",
    "print(f\"After tokenization, comments:\\n {prep_comments.tokens[test_index]}\")\n",
    "print(f\"After lemmatization, comments:\\n {prep_comments.lemmas[test_index]}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "### Document term matrix\n",
    "There is a power law distribution of tokens in all documents. This is also called the Zipf's law. So, in order to treat tokens on equal footing for language analysis, we weigh the tokens.\n",
    "The weights for each token are consolidated in something called as a document term matrix. We will see how we give weights to tokens taking note of Zipf's law.\n",
    "\n",
    "From wikipedia: [[https://en.wikipedia.org/wiki/Document-term_matrix]]\n",
    "\"\n",
    "A document-term matrix is a mathematical matrix that describes the frequency of terms that occur in a collection of documents.\n",
    "In a document-term matrix, rows correspond to documents in the collection and columns correspond to terms.\n",
    "This matrix is a specific instance of a document-feature matrix where \"features\" may refer to other properties of a document besides terms.\n",
    "It is also common to encounter the transpose, or term-document matrix where documents are the columns and terms are the rows.\n",
    "They are useful in the field of natural language processing and computational text analysis.\n",
    "\"\n",
    "* We study the document frequency in two different ways.\n",
    "(a) Term Frequency (TF, DF)\n",
    "(b) Term Frequency-Inverse Document Frequency (TF-IDF)\n",
    "\"\n",
    "Term frequency\n",
    "Suppose we have a set of English text documents and wish to rank them by which document is more relevant to the query, \"the brown cow\". A simple way to start out is by eliminating documents that do not contain all three words \"the\", \"brown\", and \"cow\", but this still leaves many documents. To further distinguish them, we might count the number of times each term occurs in each document; the number of times a term occurs in a document is called its term frequency. However, in the case where the length of documents varies greatly, adjustments are often made (see definition below). The first form of term weighting is due to Hans Peter Luhn (1957) which may be summarized as:\n",
    "    'The weight of a term that occurs in a document is simply proportional to the term frequency.'\n",
    "\n",
    "Inverse document frequency\n",
    "Because the term \"the\" is so common, term frequency will tend to incorrectly emphasize documents which happen to use the word \"the\" more frequently, without giving enough weight to the more meaningful terms \"brown\" and \"cow\". The term \"the\" is not a good keyword to distinguish relevant and non-relevant documents and terms, unlike the less-common words \"brown\" and \"cow\". Hence, an inverse document frequency factor is incorporated which diminishes the weight of terms that occur very frequently in the document set and increases the weight of terms that occur rarely.\n",
    "\n",
    "Karen Spärck Jones (1972) conceived a statistical interpretation of term-specificity called Inverse Document Frequency (idf), which became a cornerstone of term weighting:\n",
    "\n",
    "'The specificity of a term can be quantified as an inverse function of the number of documents in which it occurs.'\n",
    "\"\n",
    "\n",
    "### TF-IDF weights more words that occur frequently but in less number of documents. This seems to skew ranking towards advertisement like reviews.  Based on observations in our use case, it is not clear if such counting is relevant for this project.\n",
    "* We will use a simple count vectorizer to create the document term matrix. i.e. we simply count the number of occurances of each token and leave it at that."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "### Term Frequency on our data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "prep_comments.get_term_ranking(items='tokens', score_type='df')\n",
    "prep_comments.df_ranking['tokens'][:10]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "### Term Frequency-Inverse Document Frequency on our document\n",
    "* Note how the ranking now is different from Term Frequency ranking above\n",
    "* Note also that the terms that are ranked high *seem* to indicate advertisement words, product descriptions etc.\n",
    "    * This is why we chose not to go with TF-IDF document matrix formulation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "prep_comments.get_term_ranking(items='tokens', score_type='tfidf')\n",
    "prep_comments.tfidf_ranking['tokens'][:10]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "### Plot of TF, TF-IDF for our document\n",
    "* The frequency of words is inversely proportional to their rank. (Zipf's law)\n",
    "    * i.e. if the ranking of a token is low (low number), the frequency of that token is high (fairly obvious)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "### Term Frequency - Zipf's law plot"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "plt.figure(figsize=(10,6))\n",
    "plt.plot([x[0] for x in prep_comments.df_ranking['tokens']])\n",
    "plt.title('Document frequency ranking')\n",
    "plt.ylabel(\"Document frequency\")\n",
    "plt.xlabel(\"Term ranking\")\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "### Term Frequency LOG-LOG plot\n",
    "* Power law yields a straight line in log-log plot"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "# we can use a log-log scale to observe more clearly the power-law distribution (Zipf's law)\n",
    "plt.figure(figsize=(10,6))\n",
    "plt.loglog([x[0] for x in prep_comments.df_ranking['tokens']])\n",
    "plt.title('Document frequency ranking (log-log)')\n",
    "plt.ylabel(\"log document frequency\")\n",
    "plt.xlabel(\"log term ranking\")\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "### Term Frequency-Inverse Document Frequency plot"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "plt.figure(figsize=(10,6))\n",
    "plt.plot([x[0] for x in prep_comments.tfidf_ranking['tokens']])\n",
    "plt.title('Tf-idf ranking')\n",
    "plt.ylabel(\"tf-idf\")\n",
    "plt.xlabel(\"Term ranking\")\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "plt.figure(figsize=(10,6))\n",
    "# add 1 to prevent taking log of zero\n",
    "plt.loglog([1+x[0] for x in prep_comments.tfidf_ranking['tokens']])\n",
    "plt.title('Tf-idf ranking (log-log)')\n",
    "plt.ylabel(\"log(tf-idf)\")\n",
    "plt.xlabel(\"log(Term ranking)\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "### Use CountVectorizer() for creating document term matrix for our project\n",
    "* We apply no additional preprocessing as we have already \"pre-processed\" the comments (tokenization, removal of stop words, removal of punctuations etc.)\n",
    "* We generate unigrams, i.e. each word as its own (in contrast to two words at a time joined together) for EDA purposes\n",
    "    * When we undertake real analysis, we do add bigrams to our token set in order to capture sentiments. This is essential because we can capture sentiments such as \"good-service\", \"bad-product\", which will not be possible if we take each word seperately.\n",
    "* In order to curb noise in our token analysis, we ignore tokens that have a frequency lower than 5 occurances in all documents\n",
    "    * These frequency of occurances values can be adjusted by experimenting with various values.\n",
    "    * In this preliminary analysis, we chose this number to show case capabilities. In production, we can decide on the exact numbers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "# simple auxiliary function to override the preprocessing done by sklearn\n",
    "def do_nothing(doc):\n",
    "    return doc\n",
    "\n",
    "# create a CountVectorizer object using our preprocessed text\n",
    "count_vectorizer = CountVectorizer(encoding='utf-8',\n",
    "                                   preprocessor=do_nothing,  # apply no additional preprocessing\n",
    "                                   tokenizer=do_nothing,     # apply no additional tokenization\n",
    "                                   lowercase=False,\n",
    "                                   strip_accents=None,\n",
    "                                   stop_words=None,\n",
    "                                   ngram_range=(1, 1),       # generate only unigrams, bigrams\n",
    "                                   analyzer='word',          # analysis at the word-level\n",
    "                                   #max_df=0.5,              # ignore tokens that have a higher document frequency (can be int or percent)\n",
    "                                   #min_df=500,                # ignore tokens that have a lowe document frequency (can be int or percent)\n",
    "                                   min_df=5,\n",
    "                                   max_features=None,        # we could impose a maximum number of vocabulary terms\n",
    "                                   )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "### Output document-term matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "# transform our preprocessed tokens into a document-term matrix\n",
    "dt_matrix = count_vectorizer.fit_transform(prep_comments.tokens)\n",
    "print(f\"Document-term matrix created with shape: {dt_matrix.shape}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "### Associate words with positions in the matrix and print them\n",
    "* The value in column \"1\" indicates the column number of the word in the document term matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "# we can access a dictionary that maps between words and positions of the document-term matrix\n",
    "# list(count_vectorizer.vocabulary_.items())[0:10]\n",
    "id_word_indexer = pd.DataFrame(count_vectorizer.vocabulary_.items())\n",
    "id_word_indexer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "## When we get all the values of a particular row using df.values, we then narrow it down to the specific column to determine the index. In our case, there are only\n",
    "## two values, index 0 corresponds to name of the token and index 1 contains the index of the token in document term matrix\n",
    "this_row = 0\n",
    "required_value_index = 1\n",
    "# find index of token product\n",
    "product_index = id_word_indexer.loc[id_word_indexer[0] == 'product'].values[this_row][required_value_index]\n",
    "service_index = id_word_indexer.loc[id_word_indexer[0] == 'service'].values[this_row][required_value_index]\n",
    "quality_index = id_word_indexer.loc[id_word_indexer[0] == 'quality'].values[this_row][required_value_index]\n",
    "price_index = id_word_indexer.loc[id_word_indexer[0] == 'price'].values[this_row][required_value_index]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "### Dictionary methods\n",
    "We do a simple analysis of our comments by looking for words that denote the topics:\n",
    "* \"product\"\n",
    "* \"service\"\n",
    "* \"quality\"\n",
    "* \"price\"\n",
    "In order to find words that correspond to above topics, we find synonyms of words above and add to our vocabulary of words. We then use this comprehensive vocabulary to extract words in each document that correspond to the above words\n",
    "\n",
    "* We use publicly available NLTK based WordNet to get synonyms of words"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "### Finding synonyms using WordNet for the words\n",
    "This is a two step process. (This could be a multi-step process, but we get enough synonym words for two step process and we stop)\n",
    "\n",
    "Step 1: Find synonyms using Wordnet for \"product. service, quality, price\"\n",
    "\n",
    "Step 2: Use the resulting synonyms as starting set of words and find \"more\" synonyms.\n",
    "\n",
    "  * Because the synonym words from the wordnet synonym finder may not be entirely suitable to be used automatically we filter the result (synonyms) obtained.\n",
    "    Particularly, some words could be used in different sense (noun, adjective etc.) and only humans could determine different classification.\n",
    "    Given that we know the domain here, we want to make sure that the synonyms we find don't add to the ambiguity.\n",
    "    * For example, one of the synonyms for \"ware\" is \"convenience\" (as in mode of convenience).\n",
    "          But this could also mean \"ease of use\", which would come under \"quality\" and not under \"product\", In order to avoid this, we manually filter out the output synonyms of wordnet results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "product_words = ['gadget', 'contraption', 'appliance', 'widget', 'equipment', 'contrivance', 'gizmo', 'product', 'merchandise', 'ware', 'gismo']\n",
    "service_words = ['service', 'assist', 'help', 'aid']\n",
    "quality_words = ['quality', 'built', 'refurbish', 'comfort', 'relief']\n",
    "price_words = ['price', 'money', 'cash', 'cheap', 'costly', 'pricey', 'discount', 'payment', 'rebate', 'cost']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "Once we find the synonym words, we use these words and search each document (i.e., each row, each comment) and count the occurances of each for simple analysis later"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "### Product synonyms\n",
    "* Only nouns are considered."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "syns_indicating_product = find_wordnet_synonyms(product_words, wn.NOUN)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "### Service synonyms"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "syns_indicating_service = find_wordnet_synonyms(service_words, wn.NOUN)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "### Quality synonyms"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "syns_indicating_quality = find_wordnet_synonyms(quality_words, wn.NOUN)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "### Price synonyms"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "syns_indicating_price = find_wordnet_synonyms(price_words, wn.NOUN)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "### Find all tokens that correspond to \"price, service, quality, product\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "tokens_indicating_product = product_words\n",
    "tokens_indicating_service = service_words\n",
    "tokens_indicating_quality = quality_words\n",
    "tokens_indicating_price = price_words"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "### Determine counts of each of the above topics in each comment made by the user.\n",
    "* This information can be used for classification of documents that talk about 'service' or 'product' for example"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "### Find all token-ids corresponding to \"product, price, quality, service\" so we can add them up"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "# vocabulary's key is the feature word and the value is the feature-word's index in the feature column ...\n",
    "service_indicator_token_ids = [v for k,v in count_vectorizer.vocabulary_.items() if k in tokens_indicating_service]\n",
    "print(f\"{len(service_indicator_token_ids)} tokens found in vocabulary indicating service, {service_indicator_token_ids}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "product_indicator_token_ids = [v for k,v in count_vectorizer.vocabulary_.items() if k in tokens_indicating_product]\n",
    "print(f\"{len(product_indicator_token_ids)} tokens found in vocabulary indicating product, {product_indicator_token_ids}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "quality_indicator_token_ids = [v for k,v in count_vectorizer.vocabulary_.items() if k in tokens_indicating_quality]\n",
    "print(f\"{len(quality_indicator_token_ids)} tokens found in vocabulary indicating quality, {quality_indicator_token_ids}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "price_indicator_token_ids = [v for k,v in count_vectorizer.vocabulary_.items() if k in tokens_indicating_price]\n",
    "print(f\"{len(price_indicator_token_ids)} tokens found in vocabulary indicating price, {price_indicator_token_ids}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "### Add up the number tokens in each document (i.e., for each row, each comment) that correspond to words \"product, price,...\" and its synonyms"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "service_indicator_counts = dt_matrix.tocsr()[:, service_indicator_token_ids]\n",
    "# for a given data, count all such tokens that indicate service and presumably, one can add this as a new column to the data itself\n",
    "service_indicator_counts = service_indicator_counts.sum(axis=1)\n",
    "service_indicator_counts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "product_indicator_counts = dt_matrix.tocsr()[:, product_indicator_token_ids]\n",
    "# for a given data, count all such tokens that indicate product and presumably, one can add this as a new column to the data itself\n",
    "product_indicator_counts = product_indicator_counts.sum(axis=1)\n",
    "np.array(product_indicator_counts).ravel()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "price_indicator_counts = dt_matrix.tocsr()[:, price_indicator_token_ids]\n",
    "# for a given data, count all such tokens that indicate price and presumably, one can add this as a new column to the data itself\n",
    "price_indicator_counts = price_indicator_counts.sum(axis=1)\n",
    "np.array(price_indicator_counts).ravel()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "quality_indicator_counts = dt_matrix.tocsr()[:, quality_indicator_token_ids]\n",
    "# for a given data, count all such tokens that indicate quality and presumably, one can add this as a new column to the data itself\n",
    "quality_indicator_counts = quality_indicator_counts.sum(axis=1)\n",
    "np.array(quality_indicator_counts).ravel()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "#### Determine counts of topics: \"service, product, quality, price\"\n",
    "* We study these because it is important to know what customers think on these topics\n",
    "* Counts of: service, product, quality, price"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "dt_matrix.toarray()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "# index 14 - service, 15-product, 16-quality, 17-price\n",
    "topic_index_name_map = {}\n",
    "topic_index_name_map[product_index] = 'product'\n",
    "topic_index_name_map[service_index] = 'service'\n",
    "topic_index_name_map[quality_index] = 'quality'\n",
    "topic_index_name_map[price_index] = 'price'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "### Check to see how many documents contains the topics words\n",
    "* For each topic word ('price', 'service' etc.), how many documents contains 0, 1, 2, ... count of those words\n",
    "* Just a sanity check to see if we counted proerly or not"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "# We want to s\n",
    "for idx in [product_index, service_index, quality_index, price_index]:\n",
    "    unique, counts = np.unique(dt_matrix.toarray()[:, idx], return_counts=True)\n",
    "    print(f\"Index:{idx}, topic: {topic_index_name_map[idx]}, values:\\n{np.asarray((unique, counts)).T}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "### Add count of topics as separate columns in data for easier analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "text_data['service'] = np.array(service_indicator_counts).ravel()\n",
    "text_data['product'] = np.array(product_indicator_counts).ravel()\n",
    "text_data['quality'] = np.array(quality_indicator_counts).ravel()\n",
    "text_data['price'] = np.array(price_indicator_counts).ravel()\n",
    "text_data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "### Let us find average rating, aggregated by 'price' of the product"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    },
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "data_agg_price = text_data.groupby(['price'], as_index=False).agg({'COMMENT': 'count', 'RATING':'mean'})\n",
    "data_agg_price\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "### let us find average rating, aggregated by number of usages of word 'service'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "data_agg_service = text_data.groupby(['service'], as_index=False).agg({'COMMENT': 'count', 'RATING':'mean'})\n",
    "data_agg_service"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "### let us find average rating, aggregated by number of usages of the word 'quality'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "data_agg_quality = text_data.groupby(['quality'], as_index=False).agg({'COMMENT': 'count', 'RATING':'mean'})\n",
    "data_agg_quality"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "text_data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "### Let us find average rating by month"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "data_agg_time = text_data.groupby(pd.Grouper(freq='M'), as_index=False).agg({'COMMENT': 'count', 'RATING':'mean'})\n",
    "data_agg_time"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "### Let us find rating aggregated by the gender of the author of the comments"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "data_agg_quality = text_data.groupby(['GENDER'], as_index=False).agg({'COMMENT': 'count', 'RATING':'mean'})\n",
    "data_agg_quality"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "### Let us find rating aggregated by product and brand name"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "data_agg_quality = text_data.groupby(['PRODUCT', 'BRAND'], as_index=False).agg({'COMMENT': 'count', 'RATING':'mean'})\n",
    "data_agg_quality"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "### A way to convert document matrix into a matrix with documents as rows and token names (instead of token id) as columns for further analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "## FOR SVM, we need to make a matrix with proper column names\n",
    "# Also, we need another column that denotes the review\n",
    "# We also need to normalize the data\n",
    "df_svm = pd.DataFrame(dt_matrix.toarray())\n",
    "df_svm.rename(columns=id_word_indexer.to_dict()[0], inplace=True)\n",
    "df_svm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "7478654ab1aa58977e1af457aae5317775386ee06614bd651985e9aec83741b6"
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
