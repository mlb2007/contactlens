{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "# Recurrent Neural Network based Sentiment Analysis\n",
    "In this we use LSTM (Long Short Term Memory) perceptron and use a neural network model to model sentiments based on inout word tokens and bigrams"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "\n",
    "%matplotlib inline\n",
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import sys\n",
    "sys.path.append('pymodules')\n",
    "\n",
    "# Gensim\n",
    "from gensim.models import Word2Vec\n",
    "# making the plot look good ...\n",
    "from adjustText import adjust_text\n",
    "from sklearn.decomposition import PCA\n",
    "\n",
    "from keras_preprocessing.sequence import pad_sequences\n",
    "from keras.preprocessing.text import Tokenizer\n",
    "from keras.utils import to_categorical\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import Dense, LSTM, Embedding,Bidirectional\n",
    "from tensorflow.keras.layers import Dropout\n",
    "\n",
    "from keras.callbacks import EarlyStopping, ModelCheckpoint\n",
    "\n",
    "from sklearn import metrics\n",
    "from sklearn.model_selection import train_test_split, ParameterGrid\n",
    "\n",
    "# this class read the raw input and tokenizes comprehensively for use with modeling\n",
    "import pymodules.read_and_tokenize as contacts_utils\n",
    "\n",
    "from timeit import default_timer as timer"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "### Read file and preprocess to generate tokens"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "filename = \"data/Master-data_Q42021.xlsx\"\n",
    "prep_comments, df = contacts_utils.read_file(filename)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "### Determine response variable called 'SENTIMENT' based on rating\n",
    "* If rating is < 3, we give a value of 0\n",
    "* If rating is 3, we give a value of 1\n",
    "* If rating is > 3, we give a value of 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "# attach sentiment, seems\n",
    "def find_sentiment(rating):\n",
    "    choices = [0, 1, 2]\n",
    "    conditions = [rating < 3, rating == 3, rating > 3]\n",
    "    senti = np.select(conditions, choices)\n",
    "    return senti\n",
    "\n",
    "df['SENTIMENT'] = df['RATING'].apply(find_sentiment).astype('category')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "### Add bigrams to the word tokens so that sentiments are expressed better by word tokens and word-pairs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "require_bigrams = True\n",
    "if require_bigrams:\n",
    "    for i in range(len(prep_comments.tokens)):\n",
    "        prep_comments.tokens[i] = prep_comments.tokens[i] + prep_comments.bigrams[i]\n",
    "\n",
    "test_index = 0\n",
    "print(f\"Comments at index[{test_index}] after addition of bigrams:\\n {prep_comments.tokens[test_index]}\")\n",
    "print(f\"Comments at index[{-1}] after addition of bigrams:\\n {prep_comments.tokens[-1]}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "## Create Dictionary\n",
    "#id2word = corpora.Dictionary(prep_comments.tokens)\n",
    "## Create Corpus: Term Document Frequency\n",
    "#corpus = [id2word.doc2bow(text) for text in prep_comments.tokens]\n",
    "#print(corpus)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "### Add the token ans sentiments are new columns and display them"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "df['TOKENS'] = prep_comments.tokens"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "dfXY = df[['TOKENS', 'SENTIMENT']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "dfXY"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "### Showcas the imbalanced nature of sentiments in the given data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "import seaborn as sns\n",
    "ax = sns.countplot(x=dfXY['SENTIMENT'], order=dfXY['SENTIMENT'].value_counts(ascending=False).index)\n",
    "abs_values = dfXY['SENTIMENT'].value_counts(ascending=False).values\n",
    "ax.bar_label(container=ax.containers[0], labels=abs_values)\n",
    "ax.set(xticklabels=['>3', '<3', '=3'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "### Highly imbalanced data\n",
    "* Because of highly imbalanced data, we need to \"balance\" by using class weights while fitting any model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "## Use Keras engine for tokenization and also for RNN"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "### Take tokenized sentences and make it all integers using keras tokenizer on already tokenized data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "# because embedding is independent of tokenization, we integerize our token based on keras tokenizer\n",
    "num_expected_unique_words = 10000\n",
    "keras_tokenizer = Tokenizer(num_expected_unique_words, split=\",\")\n",
    "keras_tokenizer.fit_on_texts(dfXY['TOKENS'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "### Test, train, validation split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "validation_reqd = True\n",
    "X = dfXY.TOKENS\n",
    "y = dfXY.SENTIMENT\n",
    "df_trainX, df_trainy, df_testX, df_testy, df_validX, df_validy = contacts_utils.split_data(X, y, validation_reqd)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "### Prepare data for inout to RNN\n",
    "* We pad the data so that the sequence length that goes into RNN is always the same (Usually this is not the case), which is set to 300\n",
    "    * This sequence length is arbitrary and can be parameterized"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "MAX_SEQ_LEN = 300\n",
    "X_train=keras_tokenizer.texts_to_sequences(df_trainX) # this converts texts into some numeric sequences\n",
    "X_train_pad=pad_sequences(X_train,maxlen=MAX_SEQ_LEN,padding='post') # this makes the length of all numeric sequences equal\n",
    "\n",
    "X_test = keras_tokenizer.texts_to_sequences(df_testX)\n",
    "X_test_pad = pad_sequences(X_test, maxlen = MAX_SEQ_LEN, padding = 'post')\n",
    "\n",
    "if validation_reqd:\n",
    "    X_val = keras_tokenizer.texts_to_sequences(df_validX)\n",
    "    X_val_pad = pad_sequences(X_val, maxlen = MAX_SEQ_LEN, padding = 'post')\n",
    "else:\n",
    "    X_val = None\n",
    "    X_val_pad = None"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "y_train = to_categorical(df_trainy.values,num_classes=3)\n",
    "y_test = to_categorical(df_testy.values, num_classes=3)\n",
    "if validation_reqd:\n",
    "    y_val = to_categorical(df_validy.values, num_classes=3)\n",
    "else:\n",
    "    y_val = None"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "### Balance imbalanced data\n",
    "Compute sentiment class weights based on training data to balance the dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "from sklearn.utils.class_weight import compute_class_weight\n",
    "y_integers = np.argmax(y_train, axis=1)\n",
    "class_weights = compute_class_weight(class_weight='balanced', classes=np.unique(y_integers), y=y_integers)\n",
    "sentiment_class_weights = dict(enumerate(class_weights))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "### Test if balancing has indeed taken place ...\n",
    "print(sentiment_class_weights)\n",
    "sns.countplot(y_integers)\n",
    "plt.show()\n",
    "print(np.bincount(y_integers))\n",
    "balance_wts = np.array([val for k, val in sentiment_class_weights.items()])\n",
    "bal = np.round(np.bincount(y_integers) * balance_wts)\n",
    "sns.countplot(bal)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "### Train gensim model to generate word embeddings\n",
    "* Word embeddings vector is of size 100\n",
    "* It is based on universal dictionary\n",
    "* Each word/token now is expressed as a vector of 100 arbitrary, deterministic features. i.e. a word is embedded in a $R^{100$ basis space"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "# train Gensim's Word2Vec model\n",
    "gensim_model = Word2Vec(sentences=prep_comments.tokens,      # corpus\n",
    "                        vector_size=100,            # embedding dimension\n",
    "                        window=4,                   # words before and after to take into consideration\n",
    "                        sg=1,                       # use skip-gram\n",
    "                        negative=5,                 # number of negative examples for each positive one\n",
    "                        alpha=0.025,                # initial learning rate\n",
    "                        min_alpha=0.0001,           # minimum learning rate\n",
    "                        epochs=10,                   # number of passes through the data\n",
    "                        min_count=1,                # words that appear less than this are removed\n",
    "                        workers=4,                  # we use 1 to ensure replicability\n",
    "                        seed=92                     # for replicability\n",
    "                        )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "### Do soem gensim validation to ensure that word embeddings have been generated"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "# extract the word embeddings from the model\n",
    "word_vectors = gensim_model.wv\n",
    "word_vectors.vectors.shape  # vocab_size x embeddings dimension"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "word_vectors_weights = gensim_model.wv.vectors\n",
    "vocab_size, embedding_size = word_vectors_weights.shape\n",
    "print(\"Vocabulary Size: {} - Embedding Dim: {}\".format(vocab_size, embedding_size))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "# Some validation on the quality of the Word2Vec model\n",
    "print(gensim_model.wv.most_similar('product', topn=3))\n",
    "print(gensim_model.wv.most_similar('price', topn=3))\n",
    "print(gensim_model.wv.most_similar('service', topn=3))\n",
    "print(gensim_model.wv.most_similar('quality', topn=3))\n",
    "print(gensim_model.wv.most_similar(positive=['comfort', 'fit'], negative=['dry'], topn=3))\n",
    "\n",
    "def word2token(word):\n",
    "    try:\n",
    "        return gensim_model.wv.key_to_index[word]\n",
    "    except KeyError:\n",
    "        return 0\n",
    "\n",
    "def token2word(token):\n",
    "    return gensim_model.wv.index_to_key[token]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "gensim_weight_matrix = np.zeros((num_expected_unique_words ,embedding_size))\n",
    "gensim_weight_matrix.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "### Encode word embeddings\n",
    "* Test key to index for word vectors, so we can go back and forth between word and its embedding"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "word_vectors[word_vectors.key_to_index['dry']]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "### Map the index of the word (obtained by keras_tokenizer, which assigned interger values to words) to its weight matrix obtained from word embedding"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "for word, index in keras_tokenizer.word_index.items():\n",
    "    if index < num_expected_unique_words: # why ? since index starts with zero\n",
    "        try:\n",
    "            word_index_in_embedding = word_vectors.key_to_index[word]\n",
    "        except KeyError:\n",
    "            gensim_weight_matrix[index] = np.zeros(embedding_size)\n",
    "        else:\n",
    "            gensim_weight_matrix[index] = word_vectors[word_index_in_embedding]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "gensim_weight_matrix.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "### RNN model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "num_sentiments  = 3"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "### Cache to store intermediate results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "#EarlyStopping and ModelCheckpoint\n",
    "es = EarlyStopping(monitor = 'val_loss', mode = 'min', verbose = 1, patience = 5)\n",
    "mc = ModelCheckpoint('./sentiment_RNN_model.h5', monitor = 'val_accuracy', mode = 'max', verbose = 1, save_best_only = True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "### The model\n",
    "* 3 hidden layers\n",
    "* loss function is categorical cross entroy as the sentiments are categorical"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "def create_RNN_model(num_sentiments, sentiment_class_weights, num_expected_unique_words, embedding_size, gensim_weight_matrix, trainX, trainy, validX, validy, testX, testy):\n",
    "    def _create_RNN_model(param_dict):\n",
    "        _epochs = param_dict['epochs']\n",
    "        _batch_size = param_dict['batch_size']\n",
    "        drop_out_factor = param_dict['drop_out_factor']\n",
    "        lstm_nodes = param_dict['lstm_nodes']\n",
    "\n",
    "        # fixed three layers ...\n",
    "        model = Sequential()\n",
    "        model.add(Embedding(input_dim = num_expected_unique_words,\n",
    "                    output_dim = embedding_size,\n",
    "                    input_length= trainX.shape[1],\n",
    "                    weights = [gensim_weight_matrix],\n",
    "                    trainable = False))\n",
    "\n",
    "        model.add(Dropout(drop_out_factor[0]))\n",
    "        model.add(Bidirectional(LSTM(lstm_nodes[0],return_sequences=True)))\n",
    "        model.add(Dropout(drop_out_factor[1]))\n",
    "        model.add(Bidirectional(LSTM(lstm_nodes[1],return_sequences=True)))\n",
    "        model.add(Dropout(drop_out_factor[2]))\n",
    "        model.add(Bidirectional(LSTM(lstm_nodes[2],return_sequences=False)))\n",
    "\n",
    "        model.add(Dense(num_sentiments, activation = 'softmax'))\n",
    "\n",
    "        model.compile(loss = 'categorical_crossentropy',\n",
    "                      optimizer = 'adam',\n",
    "                      metrics = 'accuracy')\n",
    "\n",
    "        history = model.fit(trainX, trainy,\n",
    "                            epochs = _epochs,\n",
    "                            batch_size = _batch_size,\n",
    "                            validation_data=(validX, validy),\n",
    "                            verbose = 1,\n",
    "                            callbacks= [es, mc],\n",
    "                            class_weight = sentiment_class_weights,\n",
    "                            workers=4,\n",
    "                            use_multiprocessing=True)\n",
    "\n",
    "        _, test_score = model.evaluate(testX,testy)\n",
    "        y_pred_test_raw = model.predict(testX, workers=4, use_multiprocessing=True)\n",
    "        y_test_pred = np.argmax(y_pred_test_raw, axis = 1)\n",
    "        return test_score, param_dict, history, y_test_pred, model\n",
    "\n",
    "    return _create_RNN_model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "### Model parameterization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "# parameters to be varied\n",
    "batch_size = [100]\n",
    "epochs = [10]\n",
    "drop_out_factor = [[0.2, 0.2, 0.2]]\n",
    "lstm_nodes = [[100, 200, 100]]\n",
    "\n",
    "parameters = [{'epochs': epochs,\n",
    "               'batch_size': batch_size,\n",
    "               'drop_out_factor':drop_out_factor,\n",
    "               'lstm_nodes':lstm_nodes\n",
    "               }\n",
    "              ]\n",
    "\n",
    "# make a grid out of parameter choices ...\n",
    "grid_params = ParameterGrid(parameters)\n",
    "#for x in grid_params:\n",
    "#    print(f\"===== Params ===== \\n {x}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "### Run different models with different parameterizations to choose the best mode\n",
    "* Best model is chosen based on accuracy of  validation scores"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "# func that sets up the context .. i.e. what the pipeline does, what the data input is\n",
    "RNN_model_func = create_RNN_model(num_sentiments, sentiment_class_weights, num_expected_unique_words,\n",
    "                                  embedding_size, gensim_weight_matrix,\n",
    "                                  X_train_pad, y_train, X_val_pad, y_val, X_test_pad, y_test)\n",
    "\n",
    "st_ = timer()\n",
    "# run NN model in parallel and extract results (train_score, valid_score,parameter, history of fit) as a list\n",
    "results = contacts_utils.run_parallel(RNN_model_func, num_cpus=4)(grid_params)\n",
    "\n",
    "end_ = timer()\n",
    "\n",
    "print(f\"Time taken to finish best parameter search with RNN model: {(end_-st_)/60.0} mins.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "### Get the best model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "# find the best parameters based on validation score\n",
    "# results => test_score, param_dict, history, y_test_pred, model\n",
    "best_test_score = results[0][0]\n",
    "best_params = results[0][1]\n",
    "history_data = results[0][2]\n",
    "y_test_prediction_data = results[0][3]\n",
    "best_model = results[0][4]\n",
    "for i in range(1, len(results)):\n",
    "    tscore = results[i][1]\n",
    "    param = results[i][2]\n",
    "    if tscore > best_test_score:\n",
    "        best_test_score = tscore\n",
    "        best_params = param\n",
    "        history_data = results[i][2]\n",
    "        y_test_prediction_data = results[i][3]\n",
    "        best_model = results[i][4]\n",
    "\n",
    "# output result\n",
    "print(f\"Best test score:{best_test_score}\")\n",
    "print(f\"Best params based on test score:{best_params}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "source": [
    "### Metrics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "model = best_model\n",
    "history_embedding = history_data\n",
    "y_pred = y_test_prediction_data\n",
    "\n",
    "# Confusion matrix\n",
    "y_true = np.argmax(y_test, axis = 1)\n",
    "print(metrics.classification_report(y_true, y_pred, target_names=['negative', 'neutral', 'positive']))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "### Plot loss and validation, accuracy errors"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "print(history_embedding.history)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "contacts_utils.plot_loss(history_embedding)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "contacts_utils.plot_accuracy(history_embedding)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "best_model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Input test data as shwon"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "import testdata_prep_rnn as tnn\n",
    "import pandas as pd\n",
    "filename = \"data/Master-data_Q42021.xlsx\"\n",
    "print(f\"Read sheet 'Scrubbed_data' ...\")\n",
    "df_raw = pd.read_excel(filename, sheet_name='Scrubbed_data', index_col='REVIEW_DATE')\n",
    "# to get a subset as a dataframe, use double bracket notation\n",
    "some_test_data = df_raw.iloc[:42, :]\n",
    "some_test_data.columns\n",
    "\n",
    "tks, wt = tnn.process_test_data(some_test_data, gensim_model)\n",
    "tks.shape\n",
    "#best_model.predict(tks)\n",
    "X_val_pad.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "X_val_pad.shape\n",
    "tks.shape\n",
    "wt.shape\n",
    "tp_raw = best_model.predict(tks)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "y_test_pred = np.argmax(tp_raw, axis = 1)\n",
    "y_test_pred"
   ]
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "7478654ab1aa58977e1af457aae5317775386ee06614bd651985e9aec83741b6"
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
