{
 "cells": [
  {
   "cell_type": "markdown",
   "source": [
    "# Word vector and PCA analysis of tokens"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/bmukund/miniconda3/lib/python3.8/site-packages/spacy/util.py:837: UserWarning: [W095] Model 'en_core_web_sm' (3.2.0) was trained with spaCy v3.2 and may not be 100% compatible with the current version (3.3.0). If you see errors or degraded performance, download a newer compatible model or retrain your custom model with the current spaCy version. For more details and available updates, run: python -m spacy validate\n",
      "  warnings.warn(warn_msg)\n"
     ]
    }
   ],
   "source": [
    "%matplotlib inline\n",
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import sys\n",
    "sys.path.append('pymodules')\n",
    "# Gensim\n",
    "from gensim.models import Word2Vec\n",
    "# making the plot look good ...\n",
    "from adjustText import adjust_text\n",
    "from sklearn.decomposition import PCA\n",
    "# this class read the raw input and tokenizes comprehensively for use with modeling\n",
    "import pymodules.read_and_tokenize as contacts_utils\n"
   ]
  },
  {
   "cell_type": "markdown",
   "source": [
    "### Read file and preprocess to generate tokens"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Read sheet 'Scrubbed_data' ...\n",
      "Columns:Index(['TITLE', 'COMMENTS', 'OVERALL_RATING', 'COMFORT_RATING',\n",
      "       'VISION_RATING', 'VALUE_FOR_MONEY', 'AUTHOR', 'PROS', 'CONS',\n",
      "       'ORIGINAL_SOURCE', 'REPLY_FROM_ACCUVUE', 'FINAL_PRODUCT_NAME',\n",
      "       'PRODUCT_LINK', 'WEBSITE', 'RATING', 'PRODUCT', 'BRAND'],\n",
      "      dtype='object')\n",
      "Columns dropped: ['OVERALL_RATING', 'COMFORT_RATING', 'VISION_RATING', 'VALUE_FOR_MONEY', 'PROS', 'CONS', 'ORIGINAL_SOURCE', 'REPLY_FROM_ACCUVUE', 'PRODUCT_LINK', 'WEBSITE']\n",
      " Drop the Author column and replace it with gender of author ...\n",
      "Consolidate all the comments into one column called COMMENT\n",
      "Make ratings into integers\n",
      "Tokenize data based on regex found from experimentation and common usage ...\n",
      "Comments before tokenization at index[0]:\n",
      " Acucue 2 Contact Lenses I have used these lenses for a long time and I have to say that the service from Lens.com is great and the lenses work great for my needs!  I highly recommend them!\n",
      "Comments after tokenization at index[0]:\n",
      " Acucue 2 Contact Lenses I have used these lenses for a long time and I have to say that the service from Lens.com is great and the lenses work great for my needs!  I highly recommend them!\n",
      "Comments at index[0] before basic cleaning:\n",
      " Acucue 2 Contact Lenses I have used these lenses for a long time and I have to say that the service from Lens.com is great and the lenses work great for my needs!  I highly recommend them!\n",
      "Comments at index[0] after cleaning:\n",
      " acucue 2 contact lenses i have used these lenses for a long time and i have to say that the service from lens.com is great and the lenses work great for my needs!  i highly recommend them!\n",
      "Acucue 2 Contact Lenses I have used these lenses for a long time and I have to say that the service from Lens.com is great and the lenses work great for my needs!  I highly recommend them!\n",
      "\n",
      "['acucue', '2', 'contact', 'lenses', 'i', 'have', 'used', 'these', 'lenses', 'for', 'a', 'long', 'time', 'and', 'i', 'have', 'to', 'say', 'that', 'the', 'service', 'from', 'lens', '.', 'com', 'is', 'great', 'and', 'the', 'lenses', 'work', 'great', 'for', 'my', 'needs', '!', 'i', 'highly', 'recommend', 'them', '!']\n",
      "Comments at index[0] before removal of punctuations:\n",
      " Acucue 2 Contact Lenses I have used these lenses for a long time and I have to say that the service from Lens.com is great and the lenses work great for my needs!  I highly recommend them!\n",
      "Comments at index[0] after removal of punctuation:\n",
      " ['acucue', 'contact', 'lenses', 'have', 'used', 'these', 'lenses', 'for', 'long', 'time', 'and', 'have', 'say', 'that', 'the', 'service', 'from', 'lens', 'com', 'great', 'and', 'the', 'lenses', 'work', 'great', 'for', 'needs', 'highly', 'recommend', 'them']\n",
      "Comments at index[0] before removal of stop words:\n",
      " Acucue 2 Contact Lenses I have used these lenses for a long time and I have to say that the service from Lens.com is great and the lenses work great for my needs!  I highly recommend them!\n",
      "Comments at index[0] after removal of stop words:\n",
      " ['used', 'for', 'long', 'time', 'and', 'say', 'service', 'from', 'com', 'great', 'and', 'work', 'great', 'for', 'needs', 'highly', 'recommend']\n",
      "Lemmatize tokens\n",
      "Comments at index[0] after lemmatization:\n",
      " ['use', 'for', 'long', 'time', 'and', 'say', 'service', 'from', 'com', 'great', 'and', 'work', 'great', 'for', 'need', 'highly', 'recommend']\n",
      "Build the bigram and trigram words for use with topic modeling\n"
     ]
    }
   ],
   "source": [
    "filename = \"data/Master-data_Q42021.xlsx\"\n",
    "prep_comments, df = contacts_utils.read_file(filename)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "### Add bigrams to the word tokens so that sentiments are expressed better by word tokens and word-pairs"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Comments at index[0] after addition of bigrams:\n",
      " ['used', 'for', 'long', 'time', 'and', 'say', 'service', 'from', 'com', 'great', 'and', 'work', 'great', 'for', 'needs', 'highly', 'recommend', 'used-for', 'for-long', 'long-time', 'time-and', 'and-say', 'say-service', 'service-from', 'from-com', 'com-great', 'great-and', 'and-work', 'work-great', 'great-for', 'for-needs', 'needs-highly', 'highly-recommend']\n",
      "Comments at index[-1] after addition of bigrams:\n",
      " ['buy', 'again', 'order', 'came', 'fast', 'without', 'any', 'issues', 'and', 'candy', 'nice', 'touch', 'buy-again', 'again-order', 'order-came', 'came-fast', 'fast-without', 'without-any', 'any-issues', 'issues-and', 'and-candy', 'candy-nice', 'nice-touch']\n"
     ]
    }
   ],
   "source": [
    "require_bigrams = True\n",
    "if require_bigrams:\n",
    "    for i in range(len(prep_comments.tokens)):\n",
    "        prep_comments.tokens[i] = prep_comments.tokens[i] + prep_comments.bigrams[i]\n",
    "\n",
    "test_index = 0\n",
    "print(f\"Comments at index[{test_index}] after addition of bigrams:\\n {prep_comments.tokens[test_index]}\")\n",
    "print(f\"Comments at index[{-1}] after addition of bigrams:\\n {prep_comments.tokens[-1]}\")"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "df['TOKENS'] = prep_comments.tokens"
   ]
  },
  {
   "cell_type": "markdown",
   "source": [],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "### Train gensim model to generate word embeddings\n",
    "* Word embeddings vector is of size 100\n",
    "* It is based on universal dictionary\n",
    "* Each word/token now is expressed as a vector of 100 arbitrary, deterministic features. i.e. a word is embedded in a $R^{100$ basis space"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "# train Gensim's Word2Vec model\n",
    "gensim_model = Word2Vec(sentences=prep_comments.tokens,      # corpus\n",
    "                        vector_size=100,            # embedding dimension\n",
    "                        window=4,                   # words before and after to take into consideration\n",
    "                        sg=1,                       # use skip-gram\n",
    "                        negative=5,                 # number of negative examples for each positive one\n",
    "                        alpha=0.025,                # initial learning rate\n",
    "                        min_alpha=0.0001,           # minimum learning rate\n",
    "                        epochs=10,                   # number of passes through the data\n",
    "                        min_count=1,                # words that appear less than this are removed\n",
    "                        workers=4,                  # we use 1 to ensure replicability\n",
    "                        seed=92                     # for replicability\n",
    "                        )"
   ]
  },
  {
   "cell_type": "markdown",
   "source": [
    "### Do soem gensim validation to ensure that word embeddings have been generated"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": "(33417, 100)"
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# extract the word embeddings from the model\n",
    "word_vectors = gensim_model.wv\n",
    "word_vectors.vectors.shape  # vocab_size x embeddings dimension"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Vocabulary Size: 33417 - Embedding Dim: 100\n"
     ]
    }
   ],
   "source": [
    "word_vectors_weights = gensim_model.wv.vectors\n",
    "vocab_size, embedding_size = word_vectors_weights.shape\n",
    "print(\"Vocabulary Size: {} - Embedding Dim: {}\".format(vocab_size, embedding_size))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[('product-excellent', 0.8543311953544617), ('excellent-excellent', 0.8530446887016296), ('overall', 0.8521952629089355)]\n",
      "[('reasonable', 0.8825926780700684), ('inexpensive', 0.8747667670249939), ('all-around', 0.8717246055603027)]\n",
      "[('support', 0.9275480508804321), ('services', 0.9209426641464233), ('prompt', 0.9188297986984253)]\n",
      "[('inexpensive', 0.9056944251060486), ('high', 0.8903831839561462), ('consistent', 0.8887943625450134)]\n",
      "[('and-comfort', 0.761139452457428), ('comfort-fit', 0.7605573534965515), ('clarity-and', 0.755705714225769)]\n"
     ]
    }
   ],
   "source": [
    "# Some validation on the quality of the Word2Vec model\n",
    "print(gensim_model.wv.most_similar('product', topn=3))\n",
    "print(gensim_model.wv.most_similar('price', topn=3))\n",
    "print(gensim_model.wv.most_similar('service', topn=3))\n",
    "print(gensim_model.wv.most_similar('quality', topn=3))\n",
    "print(gensim_model.wv.most_similar(positive=['comfort', 'fit'], negative=['dry'], topn=3))\n",
    "\n",
    "def word2token(word):\n",
    "    try:\n",
    "        return gensim_model.wv.key_to_index[word]\n",
    "    except KeyError:\n",
    "        return 0\n",
    "\n",
    "def token2word(token):\n",
    "    return gensim_model.wv.index_to_key[token]"
   ]
  },
  {
   "cell_type": "markdown",
   "source": [
    "### Encode word embeddings\n",
    "* Test key to index for word vectors, so we can go back and forth between word and its embedding"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": "array([-0.7954323 ,  0.5615847 , -0.00218136,  0.8565504 , -0.43889657,\n        0.06994631, -0.3329911 ,  0.81239337, -0.29025328, -0.42863807,\n        0.72759354,  0.34841654, -0.23943998, -0.6637509 , -0.60335755,\n       -0.33363494, -0.2869276 ,  0.2470238 ,  0.48334715, -1.346395  ,\n        0.7449555 , -0.34173056,  0.1461135 ,  0.30779955, -0.15033358,\n       -0.06168456,  0.35324422,  0.10303527, -0.01047978,  0.17382155,\n        0.7787949 ,  0.35796463, -0.9842913 , -0.01258018,  0.4781622 ,\n        0.02132771,  0.46524855,  0.11719359,  0.6980305 , -0.7045174 ,\n        0.25984532, -0.05479569, -0.23102288,  0.24068093,  0.21667933,\n        0.3632235 ,  0.3089358 ,  0.28809568, -0.33906525, -0.1514694 ,\n        0.83478695,  0.3354382 , -0.08321823,  0.22207376, -0.5190914 ,\n       -0.18089442, -0.06170757,  0.8930125 ,  1.0786781 , -0.49132174,\n       -1.1213441 ,  1.6627904 ,  0.12870836,  0.9439195 ,  0.23270607,\n       -0.3005672 ,  0.5539319 , -1.2049255 , -0.0146581 , -0.7343427 ,\n       -0.17266688,  0.6914893 , -0.3319293 , -0.855799  ,  0.4787138 ,\n        0.21452495,  0.10501236, -0.57850736, -0.1743854 , -0.04831451,\n        0.551464  ,  0.31743774, -0.11441197, -0.32505646, -0.33970714,\n       -0.705679  , -0.38970667,  1.0487796 ,  0.03632127, -0.14245483,\n        0.98937106,  0.58680063,  0.07438911,  0.5686906 ,  0.6063803 ,\n       -0.6406817 , -0.05825983,  0.10707092, -0.21438825, -0.16075948],\n      dtype=float32)"
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "word_vectors[word_vectors.key_to_index['dry']]"
   ]
  },
  {
   "cell_type": "markdown",
   "source": [
    "### Plot the scatter matrix of word embeddings to see relative distance of words"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "# use a PCA decomposition to visualize the embeddings in 2D\n",
    "def pca_scatterplot(model, words):\n",
    "    pca = PCA(n_components=2, random_state=92)\n",
    "    word_vectors = np.array([model[w] for w in words])\n",
    "    low_dim_emb = pca.fit_transform(word_vectors)\n",
    "    plt.figure(figsize=(21,10))\n",
    "    plt.scatter(low_dim_emb[:,0], low_dim_emb[:,1], edgecolors='blue', c='blue')\n",
    "    plt.xlabel(\"Component 1\")\n",
    "    plt.ylabel(\"Component 2\")\n",
    "\n",
    "    # get the text of the plotted words\n",
    "    texts = []\n",
    "    for word, (x,y) in zip(words, low_dim_emb):\n",
    "        texts.append(plt.text(x+0.01, y+0.01, word, rotation=0))\n",
    "\n",
    "    # adjust the position of the labels so that they dont overlap\n",
    "    adjust_text(texts)\n",
    "    # show plot\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['dryer', 'usual', 'service', 'great-service', 'shelf', 'awhile', 'disappointed']\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "KeyboardInterrupt\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Error in callback <function flush_figures at 0x7fa4995d01f0> (for post_execute):\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "KeyboardInterrupt\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# define the tokens to use in the plot\n",
    "tokens_of_interest = ['dryer', 'usual', 'service', 'great-service',  'shelf', 'awhile', 'disappointed']\n",
    "print(tokens_of_interest)\n",
    "# plot\n",
    "#pca_scatterplot(word_vectors, tokens_of_interest)\n",
    "all_tokens = prep_comments.tokens\n",
    "import itertools\n",
    "flat_list_tokens = list(itertools.chain(*all_tokens))\n",
    "all_tks = list(set(flat_list_tokens))\n",
    "pca_scatterplot(word_vectors, all_tks[:1000])"
   ]
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "7478654ab1aa58977e1af457aae5317775386ee06614bd651985e9aec83741b6"
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}