{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "# Ref:https://www.machinelearningplus.com/nlp/topic-modeling-visualization-how-to-present-results-lda-models/\n",
    "%matplotlib inline\n",
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import sys\n",
    "\n",
    "sys.path.append('pymodules')\n",
    "# for dictionary method synonym finder using wordnet\n",
    "import nltk\n",
    "nltk.download('wordnet')\n",
    "nltk.download('omw-1.4')\n",
    "\n",
    "# Gensim\n",
    "import gensim, spacy, logging, warnings\n",
    "import gensim.corpora as corpora\n",
    "from gensim.models import Word2Vec\n",
    "# making the plot look good ...\n",
    "from adjustText import adjust_text\n",
    "from sklearn.decomposition import PCA\n",
    "\n",
    "from keras_preprocessing.sequence import pad_sequences\n",
    "from keras.preprocessing.text import Tokenizer\n",
    "from keras.utils import to_categorical\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import Dense, LSTM, Embedding,Bidirectional\n",
    "from tensorflow.keras.layers import Dropout\n",
    "\n",
    "from keras.callbacks import EarlyStopping, ModelCheckpoint\n",
    "\n",
    "from sklearn import metrics\n",
    "\n",
    "# this class read the raw input and tokenizes comprehensively for use with modeling\n",
    "import pymodules.read_and_tokenize as contacts_utils"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "filename = \"data/Master-data_Q42021.xlsx\"\n",
    "prep_comments, df = contacts_utils.read_file(filename)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "# attach sentiment, seems\n",
    "def find_sentiment(rating):\n",
    "    choices = [0, 1, 2]\n",
    "    conditions = [rating < 3, rating == 3, rating > 3]\n",
    "    senti = np.select(conditions, choices)\n",
    "    return senti\n",
    "\n",
    "df['SENTIMENT'] = df['RATING'].apply(find_sentiment).astype('category')"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "require_bigrams = True\n",
    "if require_bigrams:\n",
    "    for i in range(len(prep_comments.tokens)):\n",
    "        prep_comments.tokens[i] = prep_comments.tokens[i] + prep_comments.bigrams[i]\n",
    "\n",
    "test_index = 0\n",
    "print(f\"Comments at index[{test_index}] after addition of bigrams:\\n {prep_comments.tokens[test_index]}\")\n",
    "print(f\"Comments at index[{-1}] after addition of bigrams:\\n {prep_comments.tokens[-1]}\")"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "## Create Dictionary\n",
    "#id2word = corpora.Dictionary(prep_comments.tokens)\n",
    "## Create Corpus: Term Document Frequency\n",
    "#corpus = [id2word.doc2bow(text) for text in prep_comments.tokens]\n",
    "#print(corpus)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "df['TOKENS'] = prep_comments.tokens"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "dfXY = df[['TOKENS', 'SENTIMENT']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "dfXY"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "sns.countplot(dfXY.SENTIMENT)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "### Highly imbalanced data\n",
    "* Because of highly imbalanced data, we need to \"balance\" by using class weights while fitting any model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "### take tokenized sentences and make it all integers using keras tokenizer on already tokenized data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "# because embedding is independent of tokenization, we integerize our token based on keras tokenizer\n",
    "num_expected_unique_words = 10000\n",
    "keras_tokenizer = Tokenizer(num_expected_unique_words, split=\",\")\n",
    "keras_tokenizer.fit_on_texts(dfXY['TOKENS'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "validation_reqd = True\n",
    "X = dfXY.TOKENS\n",
    "y = dfXY.SENTIMENT\n",
    "df_trainX, df_trainy, df_testX, df_testy, df_validX, df_validy = contacts_utils.split_data(X, y, validation_reqd)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "MAX_SEQ_LEN = 300\n",
    "X_train=keras_tokenizer.texts_to_sequences(df_trainX) # this converts texts into some numeric sequences\n",
    "X_train_pad=pad_sequences(X_train,maxlen=MAX_SEQ_LEN,padding='post') # this makes the length of all numeric sequences equal\n",
    "\n",
    "X_test = keras_tokenizer.texts_to_sequences(df_testX)\n",
    "X_test_pad = pad_sequences(X_test, maxlen = MAX_SEQ_LEN, padding = 'post')\n",
    "\n",
    "if validation_reqd:\n",
    "    X_val = keras_tokenizer.texts_to_sequences(df_validX)\n",
    "    X_val_pad = pad_sequences(X_val, maxlen = MAX_SEQ_LEN, padding = 'post')\n",
    "else:\n",
    "    X_val = None\n",
    "    X_val_pad = None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "y_train = to_categorical(df_trainy.values,num_classes=3)\n",
    "y_test = to_categorical(df_testy.values, num_classes=3)\n",
    "if validation_reqd:\n",
    "    y_val = to_categorical(df_validy.values, num_classes=3)\n",
    "else:\n",
    "    y_val = None"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "### Compute class weights based on training data to balance the dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "from sklearn.utils.class_weight import compute_class_weight\n",
    "y_integers = np.argmax(y_train, axis=1)\n",
    "class_weights = compute_class_weight(class_weight='balanced', classes=np.unique(y_integers), y=y_integers)\n",
    "sentiment_class_weights = dict(enumerate(class_weights))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "### Test if balancing has indeed taken place ...\n",
    "print(sentiment_class_weights)\n",
    "sns.countplot(y_integers)\n",
    "plt.show()\n",
    "print(np.bincount(y_integers))\n",
    "balance_wts = np.array([val for k, val in sentiment_class_weights.items()])\n",
    "bal = np.round(np.bincount(y_integers) * balance_wts)\n",
    "sns.countplot(bal)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "# train Gensim's Word2Vec model\n",
    "gensim_model = Word2Vec(sentences=prep_comments.tokens,      # corpus\n",
    "                        vector_size=100,            # embedding dimension\n",
    "                        window=4,                   # words before and after to take into consideration\n",
    "                        sg=1,                       # use skip-gram\n",
    "                        negative=5,                 # number of negative examples for each positive one\n",
    "                        alpha=0.025,                # initial learning rate\n",
    "                        min_alpha=0.0001,           # minimum learning rate\n",
    "                        epochs=10,                   # number of passes through the data\n",
    "                        min_count=1,                # words that appear less than this are removed\n",
    "                        workers=4,                  # we use 1 to ensure replicability\n",
    "                        seed=92                     # for replicability\n",
    "                        )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "# extract the word embeddings from the model\n",
    "word_vectors = gensim_model.wv\n",
    "word_vectors.vectors.shape  # vocab_size x embeddings dimension"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "word_vectors_weights = gensim_model.wv.vectors\n",
    "vocab_size, embedding_size = word_vectors_weights.shape\n",
    "print(\"Vocabulary Size: {} - Embedding Dim: {}\".format(vocab_size, embedding_size))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "# Some validation on the quality of the Word2Vec model\n",
    "print(gensim_model.wv.most_similar('product', topn=3))\n",
    "print(gensim_model.wv.most_similar('price', topn=3))\n",
    "print(gensim_model.wv.most_similar('service', topn=3))\n",
    "print(gensim_model.wv.most_similar('quality', topn=3))\n",
    "print(gensim_model.wv.most_similar(positive=['comfort', 'fit'], negative=['dry'], topn=3))\n",
    "\n",
    "def word2token(word):\n",
    "    try:\n",
    "        return gensim_model.wv.key_to_index[word]\n",
    "    except KeyError:\n",
    "        return 0\n",
    "\n",
    "def token2word(token):\n",
    "    return gensim_model.wv.index_to_key[token]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "gensim_weight_matrix = np.zeros((num_expected_unique_words ,embedding_size))\n",
    "gensim_weight_matrix.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "source": [
    "### Test key to index for word vectors ..."
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "word_vectors[word_vectors.key_to_index['dry']]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "### map the index of the word (obtained by keras_tokenizer, which assigned interger values to words) to its weight matrix obtained from wrod embedding"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "for word, index in keras_tokenizer.word_index.items():\n",
    "    if index < num_expected_unique_words: # why ? since index starts with zero\n",
    "        try:\n",
    "            word_index_in_embedding = word_vectors.key_to_index[word]\n",
    "        except KeyError:\n",
    "            gensim_weight_matrix[index] = np.zeros(embedding_size)\n",
    "        else:\n",
    "            gensim_weight_matrix[index] = word_vectors[word_index_in_embedding]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "gensim_weight_matrix.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "### RNN model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "num_sentiments  = 3\n",
    "print(f\"input length:{X_train_pad.shape[1]}\")\n",
    "\n",
    "model = Sequential()\n",
    "model.add(Embedding(input_dim = num_expected_unique_words,\n",
    "                    output_dim = embedding_size,\n",
    "                    input_length= X_train_pad.shape[1],\n",
    "                    weights = [gensim_weight_matrix],\n",
    "                    trainable = False))\n",
    "\n",
    "model.add(Dropout(0.2))\n",
    "model.add(Bidirectional(LSTM(100,return_sequences=True)))\n",
    "model.add(Dropout(0.2))\n",
    "model.add(Bidirectional(LSTM(200,return_sequences=True)))\n",
    "model.add(Dropout(0.2))\n",
    "model.add(Bidirectional(LSTM(100,return_sequences=False)))\n",
    "model.add(Dense(num_sentiments, activation = 'softmax'))\n",
    "\n",
    "model.compile(loss = 'categorical_crossentropy',\n",
    "              optimizer = 'adam',\n",
    "              metrics = 'accuracy')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "#EarlyStopping and ModelCheckpoint\n",
    "es = EarlyStopping(monitor = 'val_loss', mode = 'min', verbose = 1, patience = 5)\n",
    "mc = ModelCheckpoint('./sentiment_RNN_model.h5', monitor = 'val_accuracy', mode = 'max', verbose = 1, save_best_only = True)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "if validation_reqd:\n",
    "    history_embedding = model.fit(X_train_pad, y_train,\n",
    "                                  epochs = 2,\n",
    "                                  batch_size = 120,\n",
    "                                  validation_data=(X_val_pad, y_val),\n",
    "                                  verbose = 1,\n",
    "                                  callbacks= [es, mc],\n",
    "                                  class_weight = sentiment_class_weights,\n",
    "                                  workers=4,\n",
    "                                  use_multiprocessing=True)\n",
    "else:\n",
    "    history_embedding = model.fit(X_train_pad, y_train,\n",
    "                                  epochs = 2,\n",
    "                                  batch_size = 120,\n",
    "                                  verbose = 1,\n",
    "                                  callbacks= [es, mc],\n",
    "                                  class_weight = sentiment_class_weights,\n",
    "                                  workers=4,\n",
    "                                  use_multiprocessing=True)\n",
    "\n",
    "\n",
    "_, accuracy = model.evaluate(X_test_pad, y_test)\n",
    "print(f'Accuracy of model is: {accuracy}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "plt.plot(history_embedding.history['accuracy'],c='b',label='train accuracy')\n",
    "plt.plot(history_embedding.history['val_accuracy'],c='r',label='validation accuracy')\n",
    "plt.legend(loc='lower right')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "y_pred_raw =model.predict(X_test_pad, workers=4, use_multiprocessing=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "y_pred =   np.argmax(y_pred_raw, axis = 1)\n",
    "y_true = np.argmax(y_test, axis = 1)\n",
    "print(metrics.classification_report(y_pred, y_true))\n",
    "#\n",
    "#y_pred = convert_prob_to_labels(y_pred_raw)\n",
    "#print(metrics.classification_report(y_pred, y_test))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "contacts_utils.plot_loss(history_embedding)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "contacts_utils.plot_accuracy(history_embedding)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "# use a PCA decomposition to visualize the embeddings in 2D\n",
    "def pca_scatterplot(model, words):\n",
    "    pca = PCA(n_components=2, random_state=92)\n",
    "    word_vectors = np.array([model[w] for w in words])\n",
    "    low_dim_emb = pca.fit_transform(word_vectors)\n",
    "    plt.figure(figsize=(21,10))\n",
    "    plt.scatter(low_dim_emb[:,0], low_dim_emb[:,1], edgecolors='blue', c='blue')\n",
    "    plt.xlabel(\"Component 1\")\n",
    "    plt.ylabel(\"Component 2\")\n",
    "\n",
    "    # get the text of the plotted words\n",
    "    texts = []\n",
    "    for word, (x,y) in zip(words, low_dim_emb):\n",
    "        texts.append(plt.text(x+0.01, y+0.01, word, rotation=0))\n",
    "\n",
    "    # adjust the position of the labels so that they dont overlap\n",
    "    adjust_text(texts)\n",
    "    # show plot\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "# define the tokens to use in the plot\n",
    "tokens_of_interest = ['dryer', 'usual', 'service', 'great-service',  'shelf', 'awhile', 'disappointed']\n",
    "print(tokens_of_interest)\n",
    "# plot\n",
    "pca_scatterplot(word_vectors, tokens_of_interest)"
   ]
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "7478654ab1aa58977e1af457aae5317775386ee06614bd651985e9aec83741b6"
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}