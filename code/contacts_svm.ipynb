{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "%timeit\n",
    "import sys\n",
    "sys.path.append('pymodules')\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "\n",
    "# this class read the raw input and tokenizes comprehensively for use with modeling\n",
    "import pymodules.read_and_tokenize as contacts_utils\n",
    "\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.metrics import accuracy_score, mean_squared_error\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.model_selection import train_test_split, ParameterGrid\n",
    "from sklearn.metrics import accuracy_score, mean_squared_error\n",
    "\n",
    "from timeit import default_timer as timer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "filename = \"data/Master-data_Q42021.xlsx\"\n",
    "prep_comments, df = contacts_utils.read_file(filename)"
   ]
  },
  {
   "cell_type": "markdown",
   "source": [
    "### Do we need bigrams ? Trigrams ?"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "require_bigrams = True\n",
    "if require_bigrams:\n",
    "    for i in range(len(prep_comments.tokens)):\n",
    "        prep_comments.tokens[i] = prep_comments.tokens[i] + prep_comments.bigrams[i]\n",
    "\n",
    "test_index = 0\n",
    "print(f\"Comments at index[{test_index}] after addition of bigrams:\\n {prep_comments.tokens[test_index]}\")\n",
    "print(f\"Comments at index[{-1}] after addition of bigrams:\\n {prep_comments.tokens[-1]}\")"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "len(prep_comments.tokens)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "### Vectorization\n",
    "* TF-IDF weights more words that occur frequently but in lesser number of documents. This seems to skew ranking towards advertisement like reviews. In any case for SVM, we use the count of tokens to establish a baseline accuracy of model, prior to using RNN model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "# simple auxiliary function to override the preprocessing done by sklearn\n",
    "def do_nothing(doc):\n",
    "    return doc\n",
    "\n",
    "# create a CountVectorizer object using our preprocessed text\n",
    "# uni gram\n",
    "count_vectorizer = CountVectorizer(encoding='utf-8',\n",
    "                                   preprocessor=do_nothing,  # apply no additional preprocessing\n",
    "                                   tokenizer=do_nothing,     # apply no additional tokenization\n",
    "                                   lowercase=False,\n",
    "                                   strip_accents=None,\n",
    "                                   stop_words=None,\n",
    "                                   ngram_range=(1, 1),       # generate only unigrams\n",
    "                                   analyzer='word',          # analysis at the word-level\n",
    "                                   #max_df=0.5,              # ignore tokens that have a higher document frequency (can be int or percent)\n",
    "                                   #min_df=500,                # ignore tokens that have a lowe document frequency (can be int or percent)\n",
    "                                   min_df=10,\n",
    "                                   max_features=None,        # we could impose a maximum number of vocabulary terms\n",
    "                                   )\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "# transform our preprocessed tokens into a document-term matrix\n",
    "dt_matrix = count_vectorizer.fit_transform(prep_comments.tokens)\n",
    "print(f\"Document-term matrix created with shape: {dt_matrix.shape}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "# we can access a dictionary that maps between words and positions of the document-term matrix. We need this for SVM in order to make the word itself as column of dataframe (see output)\n",
    "id_word_indexer = pd.DataFrame(count_vectorizer.vocabulary_.items())\n",
    "id_word_indexer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "## FOR SVM, we need to make a matrix with proper column names\n",
    "# Also, we need another column that denotes the review\n",
    "# We also need to normalize the data\n",
    "df_svm = pd.DataFrame(dt_matrix.toarray())\n",
    "df_svm.rename(columns=id_word_indexer.to_dict()[0], inplace=True)\n",
    "df_svm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "df"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "# attach sentiment, seems\n",
    "def find_sentiment(rating):\n",
    "    choices = [0, 1, 2]\n",
    "    conditions = [rating < 3, rating == 3, rating > 3]\n",
    "    senti = np.select(conditions, choices)\n",
    "    return senti\n",
    "\n",
    "SENTIMENT_SERIES = df['RATING'].apply(find_sentiment).astype('category')\n",
    "df_svm['_SENTIMENT_'] = SENTIMENT_SERIES.values\n",
    "df_svm['_SENTIMENT_'] = df_svm['_SENTIMENT_'].astype('category')\n",
    "df_svm._SENTIMENT_.value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "X = df_svm.drop(['_SENTIMENT_'], axis=1)\n",
    "y = df_svm['_SENTIMENT_']\n",
    "validation_reqd = True\n",
    "df_trainX, df_trainy, df_testX, df_testy, df_validX, df_validy = contacts_utils.split_data(X, y, validation=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "### Null accuracy\n",
    "This is the accuracy based on the proportion of input distribution of sentiment values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "rating_counts = y.value_counts()\n",
    "dn = np.sum(rating_counts)\n",
    "\n",
    "null_rating_lt_3 = round((rating_counts[0]/dn)* 100, 3)\n",
    "null_rating_eq_3 = round((rating_counts[1]/dn)* 100, 3)\n",
    "null_rating_gt_3 = round((rating_counts[2]/dn)* 100, 3)\n",
    "print(f\"Null accuracy of sentiment rating < 3, corresponding to code 0 is: {null_rating_lt_3}%\")\n",
    "print(f\"Null accuracy of sentiment rating = 3, corresponding to code 0 is: {null_rating_eq_3}%\")\n",
    "print(f\"Null accuracy of sentiment rating > 3, corresponding to code 0 is: {null_rating_gt_3}%\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "### Normalize column data please ..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "ss = StandardScaler()\n",
    "df_trainX = ss.fit_transform(df_trainX)\n",
    "df_testX = ss.fit_transform(df_testX)\n",
    "df_validX = ss.fit_transform(df_validX)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "# https://towardsdatascience.com/hyperparameter-tuning-for-support-vector-machines-c-and-gamma-parameters-6a5097416167\n",
    "# How to choose C and gamma values ...\n",
    "# 0.0001 < gamma < 10\n",
    "# 0.1 < C < 100\n",
    "# When gamma is large, C does not matter ...\n",
    "\n",
    "def create_svm_model(trainX, trainy, validX, validy):\n",
    "    def _create_svm_model(param_dict):\n",
    "        C = param_dict['C']\n",
    "        kernel = param_dict['kernel']\n",
    "        degree = param_dict['degree']\n",
    "\n",
    "        model = SVC(C=C,\n",
    "                        kernel=kernel,\n",
    "                        degree=degree,\n",
    "                        gamma='auto',\n",
    "                        class_weight='balanced',\n",
    "                        random_state=42)\n",
    "\n",
    "\n",
    "        # fit model ...\n",
    "        history = model.fit(X=trainX, y=trainy)\n",
    "            #, validation_data = (validX,validy),\n",
    "            #                verbose=1,\n",
    "            #                workers=4,\n",
    "            #                use_multiprocessing=True)\n",
    "\n",
    "        y_pred_train = model.predict(trainX)#, workers=4, use_multiprocessing=True)\n",
    "        y_pred_validation = model.predict(validX)#, workers=4, use_multiprocessing=True)\n",
    "\n",
    "        #y_train_pred_labels = convert_prob_to_labels(y_pred_train)\n",
    "        #y_validation_pred_labels = convert_prob_to_labels(y_pred_validation)\n",
    "\n",
    "        # accuracy score used ...\n",
    "        train_score = accuracy_score(trainy, y_pred_train)\n",
    "        val_score = accuracy_score(validy, y_pred_validation)\n",
    "\n",
    "        return train_score, val_score, param_dict, history, y_pred_validation\n",
    "\n",
    "    return _create_svm_model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "# parameters to be varied\n",
    "C = [1.0] #np.logspace(0, 1, 21)\n",
    "kernel_functions = ['linear'] #('linear', 'poly', 'rbf', 'sigmoid')\n",
    "degree_vals = [1]\n",
    "\n",
    "parameters = [{'C': C,\n",
    "              'kernel': kernel_functions,\n",
    "              'degree': degree_vals\n",
    "              }\n",
    "              ]\n",
    "\n",
    "# make a grid out of parameter choices ...\n",
    "grid_params = ParameterGrid(parameters)\n",
    "[print(x) for x in grid_params]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "# func that sets up the context .. i.e. what the pipeline does, what the data input is\n",
    "svm_model_func = create_svm_model(df_trainX, df_trainy, df_validX, df_validy)\n",
    "\n",
    "st_ = timer()\n",
    "# run NN model in parallel and extract results (train_score, valid_score,parameter, history of fit) as a list\n",
    "results = contacts_utils.run_parallel(svm_model_func, num_cpus=4)(grid_params)\n",
    "\n",
    "end_ = timer()\n",
    "\n",
    "print(f\"Time taken to finish best parameter search with SVM model: {(end_-st_)/60.0} mins.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "### Why SVM ? Why not LR, Naive Bayes as NULL model ?\n",
    "* LR ans SVM are closesly related. If the idea is to know the probabilities, then LR is a good option. But if the idea is to make the *right decision* (i.e. can be expressed as ratio of likelihoods), we end up with SVM method!\n",
    "* Naive Bayes assumes *class conditional independence*. In this context, this means, given the sentiments, the features found are independent of each other. This i not true for this problem.\n",
    "* LDA is very closely related to Naive Bayes with additional assumption of Gaussian distribution of features. So we can ignore this as well.\n",
    "* What about decision trees and its friends ?\n",
    "    * No proabilistic distribution assumption on response or features\n",
    "    * Can be used in offline mode, but with addition of new data, one needs to retrain from scratch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "# find the best parameters based on validation score\n",
    "best_validation_score = results[0][1]\n",
    "best_params = results[0][2]\n",
    "plot_data = results[0][3]\n",
    "best_validation_data = results[0][4]\n",
    "for i in range(1, len(results)):\n",
    "    tscore = results[i][0]\n",
    "    vscore = results[i][1]\n",
    "    param = results[i][2]\n",
    "    if vscore > best_validation_score:\n",
    "        best_validation_score = vscore\n",
    "        best_params = param\n",
    "        plot_data = results[i][3]\n",
    "        best_validation_data = results[i][4]\n",
    "\n",
    "# output result\n",
    "print(f\"Best validation score:{best_validation_score}\")\n",
    "print(f\"Best params based on validation score:{best_params}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "from sklearn import metrics\n",
    "targetnames=['negative', 'neutral', 'positive']\n",
    "#y_pred =   np.argmax(y_pred_raw, axis = 1)\n",
    "#y_true = np.argmax(y_test, axis = 1)\n",
    "print(metrics.classification_report(df_validy, best_validation_data, target_names=targetnames))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "# multi-label plot ...\n",
    "from sklearn.metrics import confusion_matrix, ConfusionMatrixDisplay\n",
    "cm = confusion_matrix(df_validy, best_validation_data)\n",
    "print(cm)\n",
    "cm_display = ConfusionMatrixDisplay(cm).plot()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "### Multi label stats\n",
    "This function extracts Accuracy, Sensitivity, Specificity and Likelihood ratio for each of the 3 sentiment labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "accuracy, sensitivity, specificity, likelihood = contacts_utils.model_stats_all_labels(best_validation_data, df_validy)\n",
    "print(f'Accuracy of model(<3, = 3, >3) is:   {np.round(accuracy*100, 2)}%')\n",
    "print(f'Sensitivity of the test(<3, = 3, >3):{np.round(sensitivity*100,2)}%')\n",
    "print(f'Specificity of the test(<3, = 3, >3):{np.round(specificity*100,2)}%')\n",
    "print(f'Likelihood ratio(<3, = 3, >3):       {np.round(likelihood, 2)}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "# We extract the best params from above cells\n",
    "best_model_params = best_params\n",
    "\n",
    "# Instead of joining validation data and train data, we simply reload the original data and split then into\n",
    "# train and test. This is suboptimal, but being relatively fast, this is ok for now\n",
    "validation_reqd = False\n",
    "# we want the output to be onehot encoded and this makes it easier to use other activation functions\n",
    "# other than softmax for the last layer, (as this is a multi-class classification problem)\n",
    "oneh = True\n",
    "trainX, trainy, testX, testy, validX, validy = contacts_utils.split_data(X, y, validation_reqd)\n",
    "print('Train: X=%s, y=%s' % (trainX.shape, trainy.shape))\n",
    "print('Test: X=%s, y=%s' % (testX.shape, testy.shape))\n",
    "\n",
    "# Now run the model with the best params\n",
    "best_model_nn = create_svm_model(trainX, trainy, testX, testy)\n",
    "st_ = timer()\n",
    "# run NN model in parallel and extract results (train_score, valid_score,parameter, history of fit) as a list\n",
    "b_train_score, b_val_score, b_param_dict, b_history, y_labels_predicted = best_model_nn(best_model_params)\n",
    "end_ = timer()\n",
    "\n",
    "accuracy, sensitivity, specificity, likelihood = contacts_utils.model_stats_all_labels(y_labels_predicted, testy)\n",
    "print(f'Accuracy of model(<3, = 3, >3) is:   {np.round(accuracy*100, 2)}%')\n",
    "print(f'Sensitivity of the test(<3, = 3, >3):{np.round(sensitivity*100,2)}%')\n",
    "print(f'Specificity of the test(<3, = 3, >3):{np.round(specificity*100,2)}%')\n",
    "print(f'Likelihood ratio(<3, = 3, >3):       {np.round(likelihood, 2)}')"
   ]
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "7478654ab1aa58977e1af457aae5317775386ee06614bd651985e9aec83741b6"
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}