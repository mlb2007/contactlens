{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/bmukund/miniconda3/lib/python3.8/site-packages/spacy/util.py:837: UserWarning: [W095] Model 'en_core_web_sm' (3.2.0) was trained with spaCy v3.2 and may not be 100% compatible with the current version (3.3.0). If you see errors or degraded performance, download a newer compatible model or retrain your custom model with the current spaCy version. For more details and available updates, run: python -m spacy validate\n",
      "  warnings.warn(warn_msg)\n",
      "[nltk_data] Downloading package wordnet to /Users/bmukund/nltk_data...\n",
      "[nltk_data]   Package wordnet is already up-to-date!\n",
      "[nltk_data] Downloading package omw-1.4 to /Users/bmukund/nltk_data...\n",
      "[nltk_data]   Package omw-1.4 is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "# Ref:https://www.machinelearningplus.com/nlp/topic-modeling-visualization-how-to-present-results-lda-models/\n",
    "%matplotlib inline\n",
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "\n",
    "import pandas as pd\n",
    "import string\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.feature_extraction.text import CountVectorizer, TfidfVectorizer\n",
    "import sys\n",
    "from collections import Counter\n",
    "import random\n",
    "import ast\n",
    "import re\n",
    "import scipy\n",
    "\n",
    "sys.path.append('pymodules')\n",
    "# This class contains some utility functions Word2Vec, stop words etc. etc.\n",
    "import pymodules.preprocessing_class as pc\n",
    "\n",
    "# gender gueser\n",
    "import gender_guesser.detector as gd\n",
    "\n",
    "# for dictionary method synonym finder using wordnet\n",
    "import nltk\n",
    "nltk.download('wordnet')\n",
    "nltk.download('omw-1.4')\n",
    "from nltk.corpus import wordnet as wn\n",
    "\n",
    "# Gensim\n",
    "import gensim, spacy, logging, warnings\n",
    "import gensim.corpora as corpora\n",
    "#from gensim.utils import lemmatize, simple_preprocess\n",
    "from gensim.models import CoherenceModel\n",
    "from gensim.models import Word2Vec\n",
    "# making the plot look good ...\n",
    "from adjustText import adjust_text\n",
    "from sklearn.decomposition import PCA\n",
    "from collections import Counter\n",
    "\n",
    "from pprint import pprint\n",
    "from wordcloud import WordCloud, STOPWORDS\n",
    "import matplotlib.colors as mcolors\n",
    "\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import Dense, LSTM, Embedding,Bidirectional\n",
    "import tensorflow\n",
    "from tensorflow.compat.v1.keras.layers import CuDNNLSTM\n",
    "from tensorflow.keras.layers import Dropout\n",
    "\n",
    "from keras_preprocessing.sequence import pad_sequences\n",
    "from keras.preprocessing.text import Tokenizer\n",
    "\n",
    "##########\n",
    "from sklearn.preprocessing import OneHotEncoder\n",
    "from sklearn.model_selection import train_test_split, ParameterGrid\n",
    "from sklearn.metrics import accuracy_score, mean_squared_error\n",
    "from sklearn.metrics import confusion_matrix, ConfusionMatrixDisplay\n",
    "from sklearn.metrics import multilabel_confusion_matrix, f1_score\n",
    "# model/ensemble scorer ...\n",
    "from sklearn.metrics import make_scorer\n",
    "import keras\n",
    "from keras.utils import to_categorical\n",
    "from keras.layers import Flatten\n",
    "from keras.optimizers import SGD\n",
    "\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import Dense, LSTM, Embedding,Bidirectional\n",
    "import tensorflow\n",
    "from tensorflow.compat.v1.keras.layers import CuDNNLSTM\n",
    "from tensorflow.keras.layers import Dropout\n",
    "\n",
    "#EarlyStopping and ModelCheckpoint\n",
    "from keras.callbacks import EarlyStopping, ModelCheckpoint"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "outputs": [],
   "source": [
    "# for parallel run of multiple trials ...\n",
    "def run_parallel(func, num_cpus=4):\n",
    "    \"\"\"\n",
    "    A simple parallel processor\n",
    "    \"\"\"\n",
    "    mp_pool = mp.Pool(num_cpus)\n",
    "    def _run(grid_parameters):\n",
    "        result = mp_pool.map(func, grid_parameters)\n",
    "        return result\n",
    "    return _run\n",
    "\n",
    "# Ref: https://machinelearningmastery.com/how-to-develop-a-cnn-from-scratch-for-fashion-mnist-clothing-classification/\n",
    "def onehot_encode(data):\n",
    "    \"\"\"\n",
    "    one hot encoding for CNN\n",
    "    \"\"\"\n",
    "    return to_categorical(data)\n",
    "\n",
    "# load dataset\n",
    "def split_data(X, y, validation=False):\n",
    "    \"\"\"\n",
    "    load data and create validation set as well (25% of training data)\n",
    "    \"\"\"\n",
    "    trainX, testX, trainy, testy = train_test_split(X, y, test_size=0.25, random_state=42)\n",
    "    # create validation data as well\n",
    "    if validation:\n",
    "        trainX, validX, trainy, validy = train_test_split(trainX, trainy, test_size=0.25, random_state=42)\n",
    "        return trainX, trainy, testX, testy, validX, validy\n",
    "    else:\n",
    "        return trainX, trainy, testX, testy, None, None\n",
    "\n",
    "\n",
    "def plot_accuracy(model, test_str='Validation'):\n",
    "    plt.plot(model.history['accuracy'])\n",
    "    plt.plot(model.history['val_accuracy'])\n",
    "    plt.title('Model accuracy')\n",
    "    plt.ylabel('Accuracy')\n",
    "    plt.xlabel('Epoch')\n",
    "    plt.legend(['Train', test_str], loc='upper left')\n",
    "    plt.show()\n",
    "\n",
    "def plot_loss(model, test_str = 'Validation'):\n",
    "    plt.plot(model.history['loss'])\n",
    "    plt.plot(model.history['val_loss'])\n",
    "    plt.title('Model loss')\n",
    "    plt.ylabel('Loss')\n",
    "    plt.xlabel('Epoch')\n",
    "    plt.legend(['Train', test_str], loc='upper left')\n",
    "    plt.show()\n"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "outputs": [],
   "source": [
    "def model_stats_all_labels(Y_pred, Y_actual):\n",
    "    # true negatives is  C[0, 0]\n",
    "    # false negatives is C[1, 0]\n",
    "    # true positives is  C[1, 1]\n",
    "    # false positives is C[0, 1]\n",
    "    mcm = multilabel_confusion_matrix(Y_actual, Y_pred)\n",
    "    tnv = mcm[:, 0, 0]\n",
    "    tpv = mcm[:, 1, 1]\n",
    "    fnv = mcm[:, 1, 0]\n",
    "    fpv = mcm[:, 0, 1]\n",
    "\n",
    "    accuracy = (tpv+tnv)/(tpv+tnv+fpv+fnv)\n",
    "    sensitivity = tpv/(tpv+fnv)\n",
    "    specificity = tnv/(tnv+fpv)\n",
    "    denom = 1-specificity\n",
    "    likelihood = [sensitivity[i]/denom[i] if denom[i] > 0 else np.nan for i in range(len(denom))]\n",
    "\n",
    "    return accuracy, sensitivity, specificity, likelihood"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "outputs": [],
   "source": [
    "def find_wordnet_synonyms(word_list, type_of_word=None):\n",
    "    \"\"\" it is assumed that the word_list words are themselves synonyms and is given as a list (even if there is only one word\n",
    "    return lemmatized synonyms ...\n",
    "    \"\"\"\n",
    "    synonyms = set()\n",
    "    for word_to_look in word_list:\n",
    "        #print(f\"looking for synonyms of word:{word_to_look}\")\n",
    "        for syn in wn.synsets(word_to_look, pos=type_of_word):\n",
    "            for i in syn.lemmas():\n",
    "                synonyms.add(i.name())\n",
    "    #print(f\"Synonyms:\\n {synonyms}\")\n",
    "    return synonyms"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "# Function to get the first name so that we can guess the gender\n",
    "def first_name(x):\n",
    "    \"\"\"\n",
    "    We determine the first name from the given string. We also remove any digits from the name. \n",
    "    Further, we use space to split names\n",
    "    \"\"\"\n",
    "    x_split = str(x).split()\n",
    "    fname = x_split[0]\n",
    "    # remove reference to digits. Now after removal, there could be some misclassification, but that is ok ..\n",
    "    fname_p = re.sub(r'[0-9]+', \"\", fname)\n",
    "    ret_str = fname_p.capitalize()\n",
    "    return ret_str\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "filename = \"data/Master-data_Q42021.xlsx\"\n",
    "text_data_raw = pd.read_excel(filename, sheet_name='Scrubbed_data', index_col='REVIEW_DATE')\n",
    "\n",
    "# We don't need these columns\n",
    "not_needed = ['OVERALL_RATING', 'COMFORT_RATING', 'VISION_RATING', 'VALUE_FOR_MONEY', 'PROS', 'CONS', 'ORIGINAL_SOURCE', 'REPLY_FROM_ACCUVUE',\n",
    "'PRODUCT_LINK', 'WEBSITE']\n",
    "\n",
    "text_data = text_data_raw.drop(columns = not_needed, axis=1)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": "female           4003\nmale             2400\nunknown          1556\nmostly_female     464\nmostly_male       253\nandy              118\nName: GENDER, dtype: int64"
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Let us figure out the gender from the names and drop the names column\n",
    "# We use gender_guesser package.\n",
    "#text_data['AUTHOR'] = text_data['AUTHOR'].astype(str)\n",
    "gdx = gd.Detector()\n",
    "text_data['GENDER'] = text_data.AUTHOR.apply(first_name).map(lambda x: gdx.get_gender(x))\n",
    "\n",
    "# Drop the author column now\n",
    "text_data.drop(columns = ['AUTHOR'], axis=1, inplace=True)\n",
    "\n",
    "# Check the gender counts just to see how the data looks like\n",
    "text_data.GENDER.value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "# Consolidate the comments into one column\n",
    "# Comments can occur both in title and in Comment columns. \n",
    "text_data['COMMENT'] = text_data['TITLE'].astype(str).fillna(\"\") + \" \" + text_data['COMMENTS'].astype(str).fillna(\"\")\n",
    "text_data.drop(columns = ['TITLE', 'COMMENTS'], axis=1, inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "outputs": [],
   "source": [
    "# clean rating\n",
    "# replace N = No rating with 0. We do this because rating is assumed to be numeric, not categorical\n",
    "text_data['RATING'].replace('N', '0', inplace=True)\n",
    "# convert rating to integers\n",
    "text_data['RATING'] = text_data['RATING'].apply(lambda x: int(x))\n",
    "\n",
    "# attach sentiment, seems\n",
    "def find_sentiment(rating):\n",
    "    choices = [-1, 0, 1]\n",
    "    conditions = [rating < 3, rating == 3, rating > 3]\n",
    "    senti = np.select(conditions, choices)\n",
    "    return senti\n",
    "\n",
    "text_data['SENTIMENT'] = text_data['RATING'].apply(find_sentiment).astype('category')"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": "                                            FINAL_PRODUCT_NAME  RATING  \\\nREVIEW_DATE                                                              \n2021-11-01   Acuvue 2 Contact Lenses                       ...       5   \n2021-12-02   Acuvue 2 Contact Lenses                       ...       4   \n2021-12-01   Acuvue 2 Contact Lenses                       ...       4   \n2021-11-16   Acuvue 2 Contact Lenses                       ...       5   \n2021-12-08   Acuvue 2 Contact Lenses                       ...       4   \n...                                                        ...     ...   \n2021-11-29   Acuvue VITA Contact Lenses                    ...       5   \n2021-11-23                Shop Acuvue Vita 12 pack  (1.0 Box )       5   \n2021-12-27          Acuvue Vita for Astigmatism Contact Lenses       5   \n2021-10-16          Acuvue Vita for Astigmatism Contact Lenses       1   \n2021-12-05          Acuvue Vita for Astigmatism Contact Lenses       5   \n\n             PRODUCT   BRAND   GENDER  \\\nREVIEW_DATE                             \n2021-11-01   Acuvue2  Acuvue     male   \n2021-12-02   Acuvue2  Acuvue   female   \n2021-12-01   Acuvue2  Acuvue   female   \n2021-11-16   Acuvue2  Acuvue     male   \n2021-12-08   Acuvue2  Acuvue  unknown   \n...              ...     ...      ...   \n2021-11-29      Vita  Acuvue     male   \n2021-11-23      Vita  Acuvue     male   \n2021-12-27      Vita  Acuvue     male   \n2021-10-16      Vita  Acuvue  unknown   \n2021-12-05      Vita  Acuvue   female   \n\n                                                       COMMENT SENTIMENT  \nREVIEW_DATE                                                               \n2021-11-01   Acucue 2 Contact Lenses I have used these lens...         1  \n2021-12-02                      Clear vision Tends to cloud up         1  \n2021-12-01   comfort These are very hard to handle. Flimsy ...         1  \n2021-11-16   Easy to use I have been using this product for...         1  \n2021-12-08   Excellent Excellent got promised a discount of...         1  \n...                                                        ...       ...  \n2021-11-29   Truly the Best The price, fast shipping,  quic...         1  \n2021-11-23   Very Comfortable Acuvue  Vita are very comfort...         1  \n2021-12-27   Very Comfortable and Convenient Very Comfortab...         1  \n2021-10-16   Worst lenses I've worn, microscopic tears in a...        -1  \n2021-12-05   Would buy again My order came in fast without ...         1  \n\n[8794 rows x 7 columns]",
      "text/html": "<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>FINAL_PRODUCT_NAME</th>\n      <th>RATING</th>\n      <th>PRODUCT</th>\n      <th>BRAND</th>\n      <th>GENDER</th>\n      <th>COMMENT</th>\n      <th>SENTIMENT</th>\n    </tr>\n    <tr>\n      <th>REVIEW_DATE</th>\n      <th></th>\n      <th></th>\n      <th></th>\n      <th></th>\n      <th></th>\n      <th></th>\n      <th></th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>2021-11-01</th>\n      <td>Acuvue 2 Contact Lenses                       ...</td>\n      <td>5</td>\n      <td>Acuvue2</td>\n      <td>Acuvue</td>\n      <td>male</td>\n      <td>Acucue 2 Contact Lenses I have used these lens...</td>\n      <td>1</td>\n    </tr>\n    <tr>\n      <th>2021-12-02</th>\n      <td>Acuvue 2 Contact Lenses                       ...</td>\n      <td>4</td>\n      <td>Acuvue2</td>\n      <td>Acuvue</td>\n      <td>female</td>\n      <td>Clear vision Tends to cloud up</td>\n      <td>1</td>\n    </tr>\n    <tr>\n      <th>2021-12-01</th>\n      <td>Acuvue 2 Contact Lenses                       ...</td>\n      <td>4</td>\n      <td>Acuvue2</td>\n      <td>Acuvue</td>\n      <td>female</td>\n      <td>comfort These are very hard to handle. Flimsy ...</td>\n      <td>1</td>\n    </tr>\n    <tr>\n      <th>2021-11-16</th>\n      <td>Acuvue 2 Contact Lenses                       ...</td>\n      <td>5</td>\n      <td>Acuvue2</td>\n      <td>Acuvue</td>\n      <td>male</td>\n      <td>Easy to use I have been using this product for...</td>\n      <td>1</td>\n    </tr>\n    <tr>\n      <th>2021-12-08</th>\n      <td>Acuvue 2 Contact Lenses                       ...</td>\n      <td>4</td>\n      <td>Acuvue2</td>\n      <td>Acuvue</td>\n      <td>unknown</td>\n      <td>Excellent Excellent got promised a discount of...</td>\n      <td>1</td>\n    </tr>\n    <tr>\n      <th>...</th>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n    </tr>\n    <tr>\n      <th>2021-11-29</th>\n      <td>Acuvue VITA Contact Lenses                    ...</td>\n      <td>5</td>\n      <td>Vita</td>\n      <td>Acuvue</td>\n      <td>male</td>\n      <td>Truly the Best The price, fast shipping,  quic...</td>\n      <td>1</td>\n    </tr>\n    <tr>\n      <th>2021-11-23</th>\n      <td>Shop Acuvue Vita 12 pack  (1.0 Box )</td>\n      <td>5</td>\n      <td>Vita</td>\n      <td>Acuvue</td>\n      <td>male</td>\n      <td>Very Comfortable Acuvue  Vita are very comfort...</td>\n      <td>1</td>\n    </tr>\n    <tr>\n      <th>2021-12-27</th>\n      <td>Acuvue Vita for Astigmatism Contact Lenses</td>\n      <td>5</td>\n      <td>Vita</td>\n      <td>Acuvue</td>\n      <td>male</td>\n      <td>Very Comfortable and Convenient Very Comfortab...</td>\n      <td>1</td>\n    </tr>\n    <tr>\n      <th>2021-10-16</th>\n      <td>Acuvue Vita for Astigmatism Contact Lenses</td>\n      <td>1</td>\n      <td>Vita</td>\n      <td>Acuvue</td>\n      <td>unknown</td>\n      <td>Worst lenses I've worn, microscopic tears in a...</td>\n      <td>-1</td>\n    </tr>\n    <tr>\n      <th>2021-12-05</th>\n      <td>Acuvue Vita for Astigmatism Contact Lenses</td>\n      <td>5</td>\n      <td>Vita</td>\n      <td>Acuvue</td>\n      <td>female</td>\n      <td>Would buy again My order came in fast without ...</td>\n      <td>1</td>\n    </tr>\n  </tbody>\n</table>\n<p>8794 rows × 7 columns</p>\n</div>"
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# display results\n",
    "text_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "outputs": [
    {
     "data": {
      "text/plain": "'Acuvue 2 Contact Lenses                                                                                     1 box - 6 pack - 3 month supply'"
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "text_data.FINAL_PRODUCT_NAME.values[0].strip(\" \\t\")"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "### Tokenization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    },
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/var/folders/bk/93tv9tk147s32tbjbfrfb79w0000gn/T/ipykernel_42775/4173119622.py:65: DeprecationWarning: Flags not at the start of the expression '(\\n    (?:\\n      (?: ' (truncated)\n",
      "  word_re = re.compile(pattern=r\"\"\"(%s)\"\"\" % \"|\".join(regex_strings), flags=re.VERBOSE | re.I)\n"
     ]
    }
   ],
   "source": [
    "## regex for tokenization\n",
    "# Ref: http://sentiment.christopherpotts.net/code-data/happyfuntokenizing.py\n",
    "emoticon_string = r\"\"\"\n",
    "    (?:\n",
    "      [<>]?\n",
    "      [:;=8]                     # eyes\n",
    "      [\\-o\\*\\']?                 # optional nose\n",
    "      [\\)\\]\\(\\[dDpP/\\:\\}\\{@\\|\\\\] # mouth\n",
    "      |\n",
    "      [\\)\\]\\(\\[dDpP/\\:\\}\\{@\\|\\\\] # mouth\n",
    "      [\\-o\\*\\']?                 # optional nose\n",
    "      [:;=8]                     # eyes\n",
    "      [<>]?\n",
    "    )\"\"\"\n",
    "\n",
    "# The components of the tokenizer:\n",
    "regex_strings = (\n",
    "    # Phone numbers:\n",
    "    r\"\"\"\n",
    "    (?:\n",
    "      (?:            # (international)\n",
    "        \\+?[01]\n",
    "        [\\-\\s.]*\n",
    "      )?\n",
    "      (?:            # (area code)\n",
    "        [\\(]?\n",
    "        \\d{3}\n",
    "        [\\-\\s.\\)]*\n",
    "      )?\n",
    "      \\d{3}          # exchange\n",
    "      [\\-\\s.]*\n",
    "      \\d{4}          # base\n",
    "    )\"\"\"\n",
    "    ,\n",
    "    # Emoticons:\n",
    "    emoticon_string\n",
    "    ,\n",
    "    # HTML tags:\n",
    "    r\"\"\"<[^>]+>\"\"\"\n",
    "    ,\n",
    "    # Twitter username:\n",
    "    r\"\"\"(?:@[\\w_]+)\"\"\"\n",
    "    ,\n",
    "    # Twitter hashtags:\n",
    "    r\"\"\"(?:\\#+[\\w_]+[\\w\\'_\\-]*[\\w_]+)\"\"\"\n",
    "    ,\n",
    "    # Remaining word types:\n",
    "    r\"\"\"\n",
    "    (?:[a-z][a-z'\\-_]+[a-z])       # Words with apostrophes or dashes.\n",
    "    |\n",
    "    (?:[+\\-]?\\d+[,/.:-]\\d+[+\\-]?)  # Numbers, including fractions, decimals.\n",
    "    |\n",
    "    (?:[\\w_]+)                     # Words without apostrophes or dashes.\n",
    "    |\n",
    "    (?:\\.(?:\\s*\\.){1,})            # Ellipsis dots.\n",
    "    |\n",
    "    (?:\\S)                         # Everything else that isn't whitespace.\n",
    "    \"\"\",\n",
    "    r\"\"\"\n",
    "    (?x)                # set flag to allow verbose regexps (to separate logical sections of pattern and add comments)\n",
    "    \\w+(?:-\\w+)*        # preserve expressions with internal hyphens as single tokens\n",
    "    | [][.,;\"'?():-_`]  # preserve punctuation as separate tokens\n",
    "    \"\"\"\n",
    ")\n",
    "word_re = re.compile(pattern=r\"\"\"(%s)\"\"\" % \"|\".join(regex_strings), flags=re.VERBOSE | re.I)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    },
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "comments_data = text_data.COMMENT\n",
    "prep_comments = pc.RawDocs(comments_data,  # series of documents\n",
    "                  lower_case=True,  # whether to lowercase the text in the firs cleaning step\n",
    "                  stopwords='long',  # type of stopwords to initialize\n",
    "                  contraction_split=True,  # wheter to split contractions or not\n",
    "                  tokenization_pattern=word_re  # custom tokenization patter\n",
    "                  )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "outputs": [
    {
     "data": {
      "text/plain": "<pymodules.preprocessing_class.RawDocs at 0x7f9e67f3c190>"
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "prep_comments"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    },
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Document from the pandas series:\n",
      " Acucue 2 Contact Lenses I have used these lenses for a long time and I have to say that the service from Lens.com is great and the lenses work great for my needs!  I highly recommend them!\n",
      "\n",
      "-------------------------\n",
      "\n",
      "Document from preprocessing object:\n",
      " Acucue 2 Contact Lenses I have used these lenses for a long time and I have to say that the service from Lens.com is great and the lenses work great for my needs!  I highly recommend them!\n"
     ]
    }
   ],
   "source": [
    "# notice that the documents from the object are identical to the ones from the pandas series\n",
    "#comments_data\n",
    "i = 0\n",
    "print(\"Document from the pandas series:\\n\", comments_data[i])\n",
    "print(\"\\n-------------------------\\n\")\n",
    "print(\"Document from preprocessing object:\\n\", prep_comments.docs[i])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    },
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "# lower-case text, expand contractions and initialize stopwords list\n",
    "stop_lens_list = ['Lens', 'lens', 'Contact-lens', 'Contact-Lens', 'lenses', 'Lenses', 'Contact-Lenses', 'Contact-lenses', 'contact-lens', 'contact-lenses', 'acucue', 'acuvue', 'Acuvue', 'pack',\n",
    "                  'box', 'Pack', 'Box', 'Moist', 'moist', 'month', 'trial']\n",
    "prep_comments.basic_cleaning(custom_stopwords_list = stop_lens_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Acucue 2 Contact Lenses I have used these lenses for a long time and I have to say that the service from Lens.com is great and the lenses work great for my needs!  I highly recommend them!\n",
      "\n",
      "acucue 2 contact lenses i have used these lenses for a long time and i have to say that the service from lens.com is great and the lenses work great for my needs!  i highly recommend them!\n"
     ]
    }
   ],
   "source": [
    "# explore an example after the basic cleaning has been applied\n",
    "i = 0\n",
    "print(comments_data[i])\n",
    "print()\n",
    "print(prep_comments.docs[i])"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "outputs": [],
   "source": [
    "# now we can split the documents into tokens\n",
    "prep_comments.tokenize_text()"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Acucue 2 Contact Lenses I have used these lenses for a long time and I have to say that the service from Lens.com is great and the lenses work great for my needs!  I highly recommend them!\n",
      "\n",
      "['acucue', '2', 'contact', 'lenses', 'i', 'have', 'used', 'these', 'lenses', 'for', 'a', 'long', 'time', 'and', 'i', 'have', 'to', 'say', 'that', 'the', 'service', 'from', 'lens', '.', 'com', 'is', 'great', 'and', 'the', 'lenses', 'work', 'great', 'for', 'my', 'needs', '!', 'i', 'highly', 'recommend', 'them', '!']\n"
     ]
    }
   ],
   "source": [
    "i = 0\n",
    "print(comments_data[i])\n",
    "print()\n",
    "print(prep_comments.tokens[i])"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "outputs": [
    {
     "data": {
      "text/plain": "'!\"#$%&\\'()*+,./:;<=>?@[\\\\]^_`{|}~'"
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "punctuation = string.punctuation\n",
    "punctuation = punctuation.replace(\"-\", \"\") # remove the hyphen from the punctuation string\n",
    "punctuation"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "outputs": [],
   "source": [
    "prep_comments.token_clean(length=2,                 # remove tokens with less than this number of characters\n",
    "                 punctuation=punctuation,           # remove custom list of punctuation characters\n",
    "                 numbers = True                     # remove numbers\n",
    "                 )"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Would buy again My order came in fast without any issues, and the candy in the box was a nice touch 😄 \n",
      "\n",
      "['would', 'buy', 'again', 'order', 'came', 'fast', 'without', 'any', 'issues', 'and', 'the', 'candy', 'the', 'box', 'was', 'nice', 'touch']\n"
     ]
    }
   ],
   "source": [
    "i = -1\n",
    "print(comments_data[i])\n",
    "print()\n",
    "print(prep_comments.tokens[i])"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "outputs": [],
   "source": [
    "# get the list of stopwords provided earlier\n",
    "#print(sorted(prep_comments.stopwords))"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "outputs": [],
   "source": [
    "# we need to specificy that we want to remove the stopwords from the \"tokens\"\n",
    "prep_comments.stopword_remove('tokens')"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Would buy again My order came in fast without any issues, and the candy in the box was a nice touch 😄 \n",
      "\n",
      "['buy', 'order', 'came', 'fast', 'without', 'issues', 'candy', 'nice', 'touch']\n"
     ]
    }
   ],
   "source": [
    "i = -1\n",
    "print(comments_data[i])\n",
    "print()\n",
    "print(prep_comments.tokens[i])"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "outputs": [],
   "source": [
    "# apply lemmatization to all documents (takes a very long time so we will avoid it for now)\n",
    "prep_comments.lemmatize()"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Would buy again My order came in fast without any issues, and the candy in the box was a nice touch 😄 \n",
      "\n",
      "['buy', 'order', 'came', 'fast', 'without', 'issues', 'candy', 'nice', 'touch']\n",
      "\n",
      "['buy', 'order', 'come', 'fast', 'without', 'issue', 'candy', 'nice', 'touch']\n"
     ]
    }
   ],
   "source": [
    "# compare all versions of the same raw sentences\n",
    "i = -1\n",
    "print(comments_data[i])\n",
    "print()\n",
    "print(prep_comments.tokens[i])\n",
    "print()\n",
    "print(prep_comments.lemmas[i])"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "outputs": [],
   "source": [
    "# Create Dictionary\n",
    "id2word = corpora.Dictionary(prep_comments.tokens)\n",
    "# Create Corpus: Term Document Frequency\n",
    "corpus = [id2word.doc2bow(text) for text in prep_comments.tokens]"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "outputs": [],
   "source": [
    "text_data['TOKENS'] = prep_comments.tokens"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "outputs": [
    {
     "data": {
      "text/plain": "                                            FINAL_PRODUCT_NAME  RATING  \\\nREVIEW_DATE                                                              \n2021-11-01   Acuvue 2 Contact Lenses                       ...       5   \n2021-12-02   Acuvue 2 Contact Lenses                       ...       4   \n2021-12-01   Acuvue 2 Contact Lenses                       ...       4   \n2021-11-16   Acuvue 2 Contact Lenses                       ...       5   \n2021-12-08   Acuvue 2 Contact Lenses                       ...       4   \n...                                                        ...     ...   \n2021-11-29   Acuvue VITA Contact Lenses                    ...       5   \n2021-11-23                Shop Acuvue Vita 12 pack  (1.0 Box )       5   \n2021-12-27          Acuvue Vita for Astigmatism Contact Lenses       5   \n2021-10-16          Acuvue Vita for Astigmatism Contact Lenses       1   \n2021-12-05          Acuvue Vita for Astigmatism Contact Lenses       5   \n\n             PRODUCT   BRAND   GENDER  \\\nREVIEW_DATE                             \n2021-11-01   Acuvue2  Acuvue     male   \n2021-12-02   Acuvue2  Acuvue   female   \n2021-12-01   Acuvue2  Acuvue   female   \n2021-11-16   Acuvue2  Acuvue     male   \n2021-12-08   Acuvue2  Acuvue  unknown   \n...              ...     ...      ...   \n2021-11-29      Vita  Acuvue     male   \n2021-11-23      Vita  Acuvue     male   \n2021-12-27      Vita  Acuvue     male   \n2021-10-16      Vita  Acuvue  unknown   \n2021-12-05      Vita  Acuvue   female   \n\n                                                       COMMENT SENTIMENT  \\\nREVIEW_DATE                                                                \n2021-11-01   Acucue 2 Contact Lenses I have used these lens...         1   \n2021-12-02                      Clear vision Tends to cloud up         1   \n2021-12-01   comfort These are very hard to handle. Flimsy ...         1   \n2021-11-16   Easy to use I have been using this product for...         1   \n2021-12-08   Excellent Excellent got promised a discount of...         1   \n...                                                        ...       ...   \n2021-11-29   Truly the Best The price, fast shipping,  quic...         1   \n2021-11-23   Very Comfortable Acuvue  Vita are very comfort...         1   \n2021-12-27   Very Comfortable and Convenient Very Comfortab...         1   \n2021-10-16   Worst lenses I've worn, microscopic tears in a...        -1   \n2021-12-05   Would buy again My order came in fast without ...         1   \n\n                                                        TOKENS  \nREVIEW_DATE                                                     \n2021-11-01   [contact, used, time, service, com, great, wor...  \n2021-12-02                       [clear, vision, tends, cloud]  \n2021-12-01   [comfort, hard, handle, flimsy, comfortable, e...  \n2021-11-16        [easy, use, using, product, years, reliable]  \n2021-12-08   [excellent, excellent, got, promised, discount...  \n...                                                        ...  \n2021-11-29   [truly, best, price, fast, shipping, quick, ve...  \n2021-11-23   [comfortable, vita, comfortable, wear, fact, c...  \n2021-12-27   [comfortable, convenient, comfortable, conveni...  \n2021-10-16   [worst, worn, microscopic, tears, week, irrita...  \n2021-12-05   [buy, order, came, fast, without, issues, cand...  \n\n[8794 rows x 8 columns]",
      "text/html": "<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>FINAL_PRODUCT_NAME</th>\n      <th>RATING</th>\n      <th>PRODUCT</th>\n      <th>BRAND</th>\n      <th>GENDER</th>\n      <th>COMMENT</th>\n      <th>SENTIMENT</th>\n      <th>TOKENS</th>\n    </tr>\n    <tr>\n      <th>REVIEW_DATE</th>\n      <th></th>\n      <th></th>\n      <th></th>\n      <th></th>\n      <th></th>\n      <th></th>\n      <th></th>\n      <th></th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>2021-11-01</th>\n      <td>Acuvue 2 Contact Lenses                       ...</td>\n      <td>5</td>\n      <td>Acuvue2</td>\n      <td>Acuvue</td>\n      <td>male</td>\n      <td>Acucue 2 Contact Lenses I have used these lens...</td>\n      <td>1</td>\n      <td>[contact, used, time, service, com, great, wor...</td>\n    </tr>\n    <tr>\n      <th>2021-12-02</th>\n      <td>Acuvue 2 Contact Lenses                       ...</td>\n      <td>4</td>\n      <td>Acuvue2</td>\n      <td>Acuvue</td>\n      <td>female</td>\n      <td>Clear vision Tends to cloud up</td>\n      <td>1</td>\n      <td>[clear, vision, tends, cloud]</td>\n    </tr>\n    <tr>\n      <th>2021-12-01</th>\n      <td>Acuvue 2 Contact Lenses                       ...</td>\n      <td>4</td>\n      <td>Acuvue2</td>\n      <td>Acuvue</td>\n      <td>female</td>\n      <td>comfort These are very hard to handle. Flimsy ...</td>\n      <td>1</td>\n      <td>[comfort, hard, handle, flimsy, comfortable, e...</td>\n    </tr>\n    <tr>\n      <th>2021-11-16</th>\n      <td>Acuvue 2 Contact Lenses                       ...</td>\n      <td>5</td>\n      <td>Acuvue2</td>\n      <td>Acuvue</td>\n      <td>male</td>\n      <td>Easy to use I have been using this product for...</td>\n      <td>1</td>\n      <td>[easy, use, using, product, years, reliable]</td>\n    </tr>\n    <tr>\n      <th>2021-12-08</th>\n      <td>Acuvue 2 Contact Lenses                       ...</td>\n      <td>4</td>\n      <td>Acuvue2</td>\n      <td>Acuvue</td>\n      <td>unknown</td>\n      <td>Excellent Excellent got promised a discount of...</td>\n      <td>1</td>\n      <td>[excellent, excellent, got, promised, discount...</td>\n    </tr>\n    <tr>\n      <th>...</th>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n    </tr>\n    <tr>\n      <th>2021-11-29</th>\n      <td>Acuvue VITA Contact Lenses                    ...</td>\n      <td>5</td>\n      <td>Vita</td>\n      <td>Acuvue</td>\n      <td>male</td>\n      <td>Truly the Best The price, fast shipping,  quic...</td>\n      <td>1</td>\n      <td>[truly, best, price, fast, shipping, quick, ve...</td>\n    </tr>\n    <tr>\n      <th>2021-11-23</th>\n      <td>Shop Acuvue Vita 12 pack  (1.0 Box )</td>\n      <td>5</td>\n      <td>Vita</td>\n      <td>Acuvue</td>\n      <td>male</td>\n      <td>Very Comfortable Acuvue  Vita are very comfort...</td>\n      <td>1</td>\n      <td>[comfortable, vita, comfortable, wear, fact, c...</td>\n    </tr>\n    <tr>\n      <th>2021-12-27</th>\n      <td>Acuvue Vita for Astigmatism Contact Lenses</td>\n      <td>5</td>\n      <td>Vita</td>\n      <td>Acuvue</td>\n      <td>male</td>\n      <td>Very Comfortable and Convenient Very Comfortab...</td>\n      <td>1</td>\n      <td>[comfortable, convenient, comfortable, conveni...</td>\n    </tr>\n    <tr>\n      <th>2021-10-16</th>\n      <td>Acuvue Vita for Astigmatism Contact Lenses</td>\n      <td>1</td>\n      <td>Vita</td>\n      <td>Acuvue</td>\n      <td>unknown</td>\n      <td>Worst lenses I've worn, microscopic tears in a...</td>\n      <td>-1</td>\n      <td>[worst, worn, microscopic, tears, week, irrita...</td>\n    </tr>\n    <tr>\n      <th>2021-12-05</th>\n      <td>Acuvue Vita for Astigmatism Contact Lenses</td>\n      <td>5</td>\n      <td>Vita</td>\n      <td>Acuvue</td>\n      <td>female</td>\n      <td>Would buy again My order came in fast without ...</td>\n      <td>1</td>\n      <td>[buy, order, came, fast, without, issues, cand...</td>\n    </tr>\n  </tbody>\n</table>\n<p>8794 rows × 8 columns</p>\n</div>"
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "text_data"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "outputs": [],
   "source": [
    "dfXY = text_data[['TOKENS', 'SENTIMENT']]\n",
    "#tks = dfXY['TOKENS'].apply(lambda x: \",\".join(x))\n",
    "#tks"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "### take tokenized sentences and make it all integers using keras tokenizer on already tokenized data"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "outputs": [],
   "source": [
    "# because embedding is independent of tokenization, we integerize our token based on keras tokenizer\n",
    "num_expected_unique_words = 10000\n",
    "keras_tokenizer = Tokenizer(num_expected_unique_words, split=\",\")\n",
    "keras_tokenizer.fit_on_texts(dfXY['TOKENS'])"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "outputs": [],
   "source": [
    "validation_reqd = True\n",
    "X = dfXY.TOKENS\n",
    "y = dfXY.SENTIMENT\n",
    "df_trainX, df_trainy, df_testX, df_testy, df_validX, df_validy = split_data(X, y, validation_reqd)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "outputs": [],
   "source": [
    "MAX_SEQ_LEN = 300\n",
    "X_train=keras_tokenizer.texts_to_sequences(df_trainX) # this converts texts into some numeric sequences\n",
    "X_train_pad=pad_sequences(X_train,maxlen=MAX_SEQ_LEN,padding='post') # this makes the length of all numeric sequences equal\n",
    "\n",
    "X_test = keras_tokenizer.texts_to_sequences(df_testX)\n",
    "X_test_pad = pad_sequences(X_test, maxlen = MAX_SEQ_LEN, padding = 'post')\n",
    "\n",
    "if validation_reqd:\n",
    "    X_val = keras_tokenizer.texts_to_sequences(df_validX)\n",
    "    X_val_pad = pad_sequences(X_val, maxlen = MAX_SEQ_LEN, padding = 'post')"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "outputs": [],
   "source": [
    "y_train = to_categorical(df_trainy.values,num_classes=3)\n",
    "y_test = to_categorical(df_testy.values, num_classes=3)\n",
    "if validation_reqd:\n",
    "    y_val = to_categorical(df_validy.values, num_classes=3)\n"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "outputs": [],
   "source": [
    "# train Gensim's Word2Vec model\n",
    "gensim_model = Word2Vec(sentences=prep_comments.tokens,      # corpus\n",
    "                        vector_size=100,            # embedding dimension\n",
    "                        window=4,                   # words before and after to take into consideration\n",
    "                        sg=1,                       # use skip-gram\n",
    "                        negative=5,                 # number of negative examples for each positive one\n",
    "                        alpha=0.025,                # initial learning rate\n",
    "                        min_alpha=0.0001,           # minimum learning rate\n",
    "                        epochs=10,                   # number of passes through the data\n",
    "                        min_count=1,                # words that appear less than this are removed\n",
    "                        workers=4,                  # we use 1 to ensure replicability\n",
    "                        seed=92                     # for replicability\n",
    "                        )"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "outputs": [
    {
     "data": {
      "text/plain": "(3866, 100)"
     },
     "execution_count": 40,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# extract the word embeddings from the model\n",
    "word_vectors = gensim_model.wv\n",
    "word_vectors.vectors.shape  # vocab_size x embeddings dimension"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Vocabulary Size: 3866 - Embedding Dim: 100\n"
     ]
    }
   ],
   "source": [
    "word_vectors_weights = gensim_model.wv.vectors\n",
    "vocab_size, embedding_size = word_vectors_weights.shape\n",
    "print(\"Vocabulary Size: {} - Embedding Dim: {}\".format(vocab_size, embedding_size))"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[('complaints', 0.8697736859321594), ('products', 0.8516852855682373), ('organization', 0.8452100157737732)]\n",
      "[('inexpensive', 0.8836637139320374), ('competition', 0.8827552199363708), ('pricing', 0.8810729384422302)]\n",
      "[('prompt', 0.9106859564781189), ('pricing', 0.9091824889183044), ('products', 0.900623083114624)]\n",
      "[('inexpensive', 0.9143714308738708), ('value', 0.8826212882995605), ('organization', 0.8796113729476929)]\n",
      "[('clarity', 0.7502287030220032), ('durability', 0.681592583656311), ('view', 0.6641735434532166)]\n"
     ]
    }
   ],
   "source": [
    "# Some validation on the quality of the Word2Vec model\n",
    "print(gensim_model.wv.most_similar('product', topn=3))\n",
    "print(gensim_model.wv.most_similar('price', topn=3))\n",
    "print(gensim_model.wv.most_similar('service', topn=3))\n",
    "print(gensim_model.wv.most_similar('quality', topn=3))\n",
    "print(gensim_model.wv.most_similar(positive=['comfort', 'fit'], negative=['dry'], topn=3))\n",
    "\n",
    "def word2token(word):\n",
    "    try:\n",
    "        return gensim_model.wv.key_to_index[word]\n",
    "    except KeyError:\n",
    "        return 0\n",
    "\n",
    "def token2word(token):\n",
    "    return gensim_model.wv.index_to_key[token]"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "outputs": [
    {
     "data": {
      "text/plain": "(10000, 100)"
     },
     "execution_count": 43,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "gensim_weight_matrix = np.zeros((num_expected_unique_words ,embedding_size))\n",
    "gensim_weight_matrix.shape"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "outputs": [
    {
     "data": {
      "text/plain": "array([-0.53521633,  0.05164455, -0.14615655,  0.00354287, -0.27384073,\n       -0.23983862, -0.55116594, -0.0630076 , -0.0606628 ,  0.28825465,\n        0.59325796, -0.26647836, -0.1836873 ,  0.05411916,  0.25851598,\n       -0.02366793,  0.11032991, -0.13819452,  0.24570134, -0.5614637 ,\n        0.16899586, -0.15074413,  0.46231315,  0.3578053 ,  0.01405786,\n        0.08570202,  0.13133995, -0.5032321 , -0.46121374,  0.1523004 ,\n        0.4295542 ,  0.04771445, -0.35763517,  0.11895806,  0.59200794,\n        0.08754861,  0.14250754, -0.13007848, -0.31203252, -0.35849765,\n        0.02207035,  0.30058676, -0.25078374, -0.12696745, -0.2252482 ,\n        0.4717039 ,  0.11393612,  0.03326302,  0.10147794,  0.45847622,\n       -0.2973906 ,  0.48349777, -0.16567683, -0.57360154,  0.00846396,\n       -0.26997232,  0.40751144, -0.12610643, -0.1239848 ,  0.07203636,\n       -0.2873029 ,  0.33211568, -0.18177925,  0.56625015,  0.07143534,\n       -0.45183152,  0.07838323,  0.25135577,  0.19661106, -0.36850464,\n       -0.5228127 ,  0.1836055 ,  0.4109355 , -0.04082282,  0.20389841,\n        0.36269036,  0.04631887,  0.06101173,  0.02282304,  0.20809256,\n        0.16033249, -0.1245624 , -0.47954333, -0.28881785, -0.22995256,\n        0.04052249, -0.00851248,  0.01832477,  0.04952211,  0.18384553,\n        0.00163553,  0.06521156, -0.6458048 , -0.06331585, -0.24277829,\n       -0.05237048,  0.54793036,  0.29450598,  0.20558104, -0.14661469],\n      dtype=float32)"
     },
     "execution_count": 44,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "word_vectors[word_vectors.key_to_index['dry']]"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "### map the index of the word (obtained by keras_tokenizer, which assigned interger values to words) to its weight matrix obtained from wrod embedding"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "outputs": [],
   "source": [
    "for word, index in keras_tokenizer.word_index.items():\n",
    "    if index < num_expected_unique_words: # why ? since index starts with zero\n",
    "        try:\n",
    "            word_index_in_embedding = word_vectors.key_to_index[word]\n",
    "        except KeyError:\n",
    "            gensim_weight_matrix[index] = np.zeros(embedding_size)\n",
    "        else:\n",
    "            gensim_weight_matrix[index] = word_vectors[word_index_in_embedding]"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "outputs": [
    {
     "data": {
      "text/plain": "(10000, 100)"
     },
     "execution_count": 46,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "gensim_weight_matrix.shape"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "### RNN model"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "outputs": [],
   "source": [
    "num_sentiments  = 3\n",
    "\n",
    "model = Sequential()\n",
    "model.add(Embedding(input_dim = num_expected_unique_words,\n",
    "                    output_dim = embedding_size,\n",
    "                    input_length= X_train_pad.shape[1],\n",
    "                    weights = [gensim_weight_matrix],trainable = False))\n",
    "model.add(Dropout(0.2))\n",
    "model.add(Bidirectional(LSTM(100,return_sequences=True)))\n",
    "model.add(Dropout(0.2))\n",
    "model.add(Bidirectional(LSTM(200,return_sequences=True)))\n",
    "model.add(Dropout(0.2))\n",
    "model.add(Bidirectional(LSTM(100,return_sequences=False)))\n",
    "model.add(Dense(num_sentiments, activation = 'softmax'))\n",
    "model.compile(loss = 'categorical_crossentropy', optimizer = 'adam', metrics = 'accuracy')"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "outputs": [],
   "source": [
    "#EarlyStopping and ModelCheckpoint\n",
    "from keras.callbacks import EarlyStopping, ModelCheckpoint\n",
    "es = EarlyStopping(monitor = 'val_loss', mode = 'min', verbose = 1, patience = 5)\n",
    "mc = ModelCheckpoint('./sentiment_RNN_model.h5', monitor = 'val_accuracy', mode = 'max', verbose = 1, save_best_only = True)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/2\n",
      "42/42 [==============================] - ETA: 0s - loss: 0.2823 - accuracy: 0.9373\n",
      "Epoch 1: val_accuracy improved from -inf to 0.95512, saving model to ./sentiment_RNN_model.h5\n",
      "42/42 [==============================] - 153s 3s/step - loss: 0.2823 - accuracy: 0.9373 - val_loss: 0.2050 - val_accuracy: 0.9551\n",
      "Epoch 2/2\n",
      "42/42 [==============================] - ETA: 0s - loss: 0.2068 - accuracy: 0.9527\n",
      "Epoch 2: val_accuracy did not improve from 0.95512\n",
      "42/42 [==============================] - 140s 3s/step - loss: 0.2068 - accuracy: 0.9527 - val_loss: 0.1871 - val_accuracy: 0.9551\n"
     ]
    }
   ],
   "source": [
    "history_embedding = model.fit(X_train_pad, y_train,\n",
    "                              epochs = 2, batch_size = 120,\n",
    "                              validation_data=(X_val_pad, y_val),\n",
    "                              verbose = 1, callbacks= [es, mc]  )"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "outputs": [
    {
     "data": {
      "text/plain": "<Figure size 432x288 with 1 Axes>",
      "image/png": "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\n"
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "plt.plot(history_embedding.history['accuracy'],c='b',label='train accuracy')\n",
    "plt.plot(history_embedding.history['val_accuracy'],c='r',label='validation accuracy')\n",
    "plt.legend(loc='lower right')\n",
    "plt.show()"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "y_pred =   np.argmax(model.predict(X_test_pad), axis  =  1)\n",
    "y_true = np.argmax(y_test, axis = 1)\n",
    "from sklearn import metrics\n",
    "print(metrics.classification_report(y_pred, y_true))"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "# use a PCA decomposition to visualize the embeddings in 2D\n",
    "def pca_scatterplot(model, words):\n",
    "    pca = PCA(n_components=2, random_state=92)\n",
    "    word_vectors = np.array([model[w] for w in words])\n",
    "    low_dim_emb = pca.fit_transform(word_vectors)\n",
    "    plt.figure(figsize=(21,10))\n",
    "    plt.scatter(low_dim_emb[:,0], low_dim_emb[:,1], edgecolors='blue', c='blue')\n",
    "    plt.xlabel(\"Component 1\")\n",
    "    plt.ylabel(\"Component 2\")\n",
    "\n",
    "    # get the text of the plotted words\n",
    "    texts = []\n",
    "    for word, (x,y) in zip(words, low_dim_emb):\n",
    "        texts.append(plt.text(x+0.01, y+0.01, word, rotation=0))\n",
    "\n",
    "    # adjust the position of the labels so that they dont overlap\n",
    "    adjust_text(texts)\n",
    "    # show plot\n",
    "    plt.show()"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "# define the tokens to use in the plot\n",
    "tokens_of_interest = ['dryer', 'usual', 'seems', 'shelf', 'awhile', 'disappointed']\n",
    "print(tokens_of_interest)\n",
    "# expand the list of tokens with all the tokens from the replacement dictionary\n",
    "#tokens_of_interest = set(tokens_of_interest) + list(replacing_dict.values()) )\n",
    "\n",
    "# plot\n",
    "pca_scatterplot(word_vectors, tokens_of_interest)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "# Get topic weights and dominant topics ------------\n",
    "from sklearn.manifold import TSNE\n",
    "from bokeh.plotting import figure, output_file, show\n",
    "from bokeh.models import Label\n",
    "from bokeh.io import output_notebook\n",
    "\n",
    "## Get topic weights\n",
    "#topic_weights = []\n",
    "#for i, row_list in enumerate(lda_model[corpus]):\n",
    "#    topic_weights.append([w for j, w in row_list[0]])\n",
    "#\n",
    "## Array of topic weights\n",
    "#arr = pd.DataFrame(topic_weights).fillna(0).values\n",
    "#\n",
    "## Keep the well separated points (optional)\n",
    "#arr = arr[np.amax(arr, axis=1) > 0.35]\n",
    "#\n",
    "## Dominant topic number in each doc\n",
    "#topic_num = np.argmax(arr, axis=1)\n",
    "\n",
    "# tSNE Dimension Reduction\n",
    "#tsne_model = TSNE(n_components=2, verbose=1, random_state=0, angle=.99, init='pca')\n",
    "#tsne_lda = tsne_model.fit_transform(word_vectors.vectors[:50])#(arr)\n",
    "\n",
    "# Plot the Topic Clusters using Bokeh\n",
    "output_notebook()\n",
    "n_topics = 4\n",
    "mycolors = np.array([color for name, color in mcolors.TABLEAU_COLORS.items()])\n",
    "plot = figure(title=\"t-SNE Clustering of {} LDA Topics\".format(n_topics), plot_width=900, plot_height=700)\n",
    "plot.scatter(x=tsne_lda[:,0], y=tsne_lda[:,1], color=mycolors[topic_num])\n",
    "show(plot)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "7478654ab1aa58977e1af457aae5317775386ee06614bd651985e9aec83741b6"
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}