{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "optional-influence",
   "metadata": {
    "id": "optional-influence",
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "# **Introduction to Word Embeddings**\n",
    "\n",
    "This tutorial illustrates several applications of word embeddings by estimating a Word2Vec model using an off-the-shelf python library (```Gensim```).\n",
    "\n",
    "Some additional resources on Word2Vec:\n",
    "- [Jurafsky & Martin (2021). Book chapter.](https://web.stanford.edu/~jurafsky/slp3/6.pdf)\n",
    "- [Mikolov et al. (2013). Original paper.](https://arxiv.org/pdf/1301.3781.pdf)\n",
    "- [Rong (2016). Additional explanation on how to train the model.](https://arxiv.org/pdf/1411.2738.pdf)\n",
    "- [Alammar (2019). Illustrated guide.](https://jalammar.github.io/illustrated-word2vec/)\n",
    "- [Gensim documentation](https://radimrehurek.com/gensim/models/word2vec.html)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e75c2330-27c2-4cf6-b7d8-312d7b60c898",
   "metadata": {
    "id": "e75c2330-27c2-4cf6-b7d8-312d7b60c898",
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "## **Setup**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bc05b753-7cb8-43b7-8ec8-680bb0f188ba",
   "metadata": {
    "id": "bc05b753-7cb8-43b7-8ec8-680bb0f188ba",
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "# ## install necessary packages\n",
    "# !pip install flashtext                  # easy phrase replacing methods\n",
    "# !pip install contractions               # expand English contractions \n",
    "# !pip install --upgrade spacy==2.2.4     # functions for lemmatizing\n",
    "# !pip install gensim==4.0.0              # word2vec estimation\n",
    "# !pip install adjustText                 # generate plots with lots of text labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "military-demonstration",
   "metadata": {
    "id": "military-demonstration",
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "import sys\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import string\n",
    "from gensim.models import Word2Vec\n",
    "import matplotlib.pyplot as plt\n",
    "from adjustText import adjust_text\n",
    "from sklearn.decomposition import PCA\n",
    "from collections import Counter\n",
    "from sklearn.preprocessing import StandardScaler"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5824485c",
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "# define paths\n",
    "data_path = \"../data/\"\n",
    "pymodules_path = \"../pymodules\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3ed59a1b",
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "# import our own code\n",
    "sys.path.append(pymodules_path)\n",
    "import preprocessing_class as pc\n",
    "import dictionary_methods as dictionary_methods"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ff8cb7d3-d86a-4fdc-acc5-7402e74b12c8",
   "metadata": {
    "id": "ff8cb7d3-d86a-4fdc-acc5-7402e74b12c8",
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "## **Off-the-shelf Word2Vec using Gensim**\n",
    "\n",
    "[Gensim](https://radimrehurek.com/gensim/index.html) is a very powerful library that contains efficient (written in ```C```) implementations of several NLP models. Word2Vec is included among these. We will start by using this library to demonstrate use cases for word embeddings."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3a7c7a62-c1ab-4aeb-a2e8-ac009d7b6c34",
   "metadata": {
    "id": "3a7c7a62-c1ab-4aeb-a2e8-ac009d7b6c34",
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "### *Load data and preprocess text*\n",
    "\n",
    "We will now load some real data over which we will estimate our word embeddings. We see that our data consists of paragraphs from the Inflation Reports produced by the Bank of England. The data starts on 1998 and ends in 2015. Reports are produced four times a year in the months of February, May, August and November."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5952f0ca",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "As a starting point, we will need to download three data files from Google Drive:\n",
    "1. [Inflation Reports data](https://drive.google.com/file/d/1o_67kmSkLjaEoYxIUkfnouC_Nm_IQJZE/view?usp=sharing) \n",
    "2. [Monetary Policy Committee minutes data](https://drive.google.com/file/d/1iCtirTJfgowx2TVJVmISNRbq690wv1vm/view?usp=sharing)\n",
    "3. [Quarterly GDP data](https://drive.google.com/file/d/1m9lTsJU2--K8mpLu2STZSS691WLZnN6h/view?usp=sharing)\n",
    "\n",
    "Once you download the two files put them in the *\"data\"* folder and keep their original names."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c9f47d7b-e1b3-4326-846b-5dd4c1a7cb99",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "c9f47d7b-e1b3-4326-846b-5dd4c1a7cb99",
    "outputId": "9c5c28d6-0531-4d32-f016-1e822085f4fb",
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "data = pd.read_csv(data_path + \"ir_data_final.txt\", sep=\"\\t\")\n",
    "data = data[['ir_date', 'paragraph']]\n",
    "data.columns = ['yearmonth', 'paragraph']\n",
    "print(data.shape)\n",
    "data.head(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a647e409",
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "# explore one of the paragraphs\n",
    "data.loc[0, \"paragraph\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d856f1d9-edd9-4bda-a56d-b4c48a37b074",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "d856f1d9-edd9-4bda-a56d-b4c48a37b074",
    "outputId": "345e1f6a-537b-47dc-e957-9923f394b7b1",
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "# check how often these reports are produced\n",
    "grouped = data.groupby(\"yearmonth\", as_index=False).size()\n",
    "print(grouped.head(5))\n",
    "print()\n",
    "print(grouped.tail(5))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e_TVUwiS1uDW",
   "metadata": {
    "id": "e_TVUwiS1uDW",
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "def apply_preprocessing(data, replacing_dict, pattern, punctuation):\n",
    "    \"\"\" Function to apply the steps from the preprocessing class in the correct\n",
    "        order to generate a term frequency matrix and the appropriate dictionaries\n",
    "    \"\"\"\n",
    "    \n",
    "    prep = pc.RawDocs(data, stopwords=\"short\", lower_case=True, contraction_split=True, tokenization_pattern=pattern)\n",
    "    prep.phrase_replace(replace_dict=replacing_dict, case_sensitive_replacing=False)\n",
    "    # lower-case text, expand contractions and initialize stopwords list\n",
    "    prep.basic_cleaning()\n",
    "    # split the documents into tokens\n",
    "    prep.tokenize_text()\n",
    "    # clean tokens\n",
    "    prep.token_clean(length=2, punctuation=punctuation, numbers=True)\n",
    "    # create document-term matrix\n",
    "    prep.dt_matrix_create(items='tokens', min_df=10, score_type='df')\n",
    "    \n",
    "    # get the vocabulary and the appropriate dictionaries to map from indices to words\n",
    "    word2idx = prep.vocabulary[\"tokens\"]\n",
    "    idx2word = {i:word for word,i in word2idx.items()}\n",
    "    vocab = list(word2idx.keys())\n",
    "    \n",
    "    return prep, word2idx, idx2word, vocab"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "hiGs2bZK198L",
   "metadata": {
    "id": "hiGs2bZK198L",
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "# define dictionary for pre-processing class with terms we want to preserve\n",
    "replacing_dict = {'monetary policy':'monetary-policy',\n",
    "                  'interest rate':'interest-rate',\n",
    "                  'interest rates':'interest-rate',\n",
    "                  'yield curve':'yield-curve',\n",
    "                  'repo rate':'repo-rate',\n",
    "                  'bond yields':'bond-yields',\n",
    "                  'real estate':'real-estate',\n",
    "                  'economic growth':'economic-growth'}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "00ee427d-4a0b-4233-99ee-b4d4f4564a0d",
   "metadata": {
    "id": "00ee427d-4a0b-4233-99ee-b4d4f4564a0d",
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "# define tokenization pattern and punctuation symbols\n",
    "pattern = r'''\n",
    "          (?x)                # set flag to allow verbose regexps (to separate logical sections of pattern and add comments)\n",
    "          \\w+(?:-\\w+)*        # word characters with internal hyphens\n",
    "          | [][.,;\"'?():-_`]  # preserve punctuation as separate tokens\n",
    "          '''\n",
    "punctuation = string.punctuation.replace(\"-\", \"\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ce3153bc-9af9-40ec-a5cb-25f2dfa2dd61",
   "metadata": {
    "id": "ce3153bc-9af9-40ec-a5cb-25f2dfa2dd61",
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "# use preprocessing class\n",
    "prep, word2idx, idx2word, vocab = apply_preprocessing(data.paragraph, replacing_dict, pattern, punctuation)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7bf4b965-5095-42d4-84ec-d9d64bd063fa",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "7bf4b965-5095-42d4-84ec-d9d64bd063fa",
    "outputId": "ecc89fd3-1e2f-4e32-fbbb-6fde765d91a5",
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "# inspect a particular tokenized document and compare to its original form\n",
    "i = 10\n",
    "print(data.paragraph[i])\n",
    "print(\"\\n ------------------------------- \\n\")\n",
    "print(prep.tokens[i])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7ddc39dd-70b8-43a2-a5e7-6a943cbdce33",
   "metadata": {
    "id": "7ddc39dd-70b8-43a2-a5e7-6a943cbdce33",
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "### *Model estimation*\n",
    "\n",
    "Now that we have our text preprocessed we can use the [Gensim](https://radimrehurek.com/gensim/) library to efficiently estimate word embeddings using word2vec."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "247cede9-6076-44c1-8977-85f6844f00d3",
   "metadata": {
    "id": "247cede9-6076-44c1-8977-85f6844f00d3",
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "# train Gensim's Word2Vec model\n",
    "gensim_model = Word2Vec(sentences=prep.tokens,      # corpus\n",
    "                        vector_size=100,            # embedding dimension\n",
    "                        window=4,                   # words before and after to take into consideration\n",
    "                        sg=1,                       # use skip-gram\n",
    "                        negative=5,                 # number of negative examples for each positive one\n",
    "                        alpha=0.025,                # initial learning rate\n",
    "                        min_alpha=0.0001,           # minimum learning rate\n",
    "                        epochs=5,                   # number of passes through the data\n",
    "                        min_count=1,                # words that appear less than this are removed\n",
    "                        workers=1,                  # we use 1 to ensure replicability\n",
    "                        seed=92                     # for replicability\n",
    "                       )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b9ca52c2-38b9-4377-9d19-c46f0c7194f2",
   "metadata": {
    "id": "b9ca52c2-38b9-4377-9d19-c46f0c7194f2",
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "# extract the word embeddings from the model\n",
    "word_vectors = gensim_model.wv\n",
    "word_vectors.vectors.shape  # vocab_size x embeddings dimension"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8eb3d717",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "There a lot of different ways in which we can use these estimated word embeddings. We will start by showing a simple way to visualize them in 2-dimensions."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8afe694f-f4ee-486a-ac1b-7b2720077a7b",
   "metadata": {
    "id": "8afe694f-f4ee-486a-ac1b-7b2720077a7b",
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "### *Visualization*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6c11c66f-3219-4f7c-a777-fe0dc1ddb220",
   "metadata": {
    "id": "6c11c66f-3219-4f7c-a777-fe0dc1ddb220",
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "# use a PCA decomposition to visualize the embeddings in 2D\n",
    "def pca_scatterplot(model, words):\n",
    "    pca = PCA(n_components=2, random_state=92)\n",
    "    word_vectors = np.array([model[w] for w in words])\n",
    "    low_dim_emb = pca.fit_transform(word_vectors)\n",
    "    plt.figure(figsize=(21,10))\n",
    "    plt.scatter(low_dim_emb[:,0], low_dim_emb[:,1], edgecolors='blue', c='blue')\n",
    "    plt.xlabel(\"Component 1\")\n",
    "    plt.ylabel(\"Component 2\")\n",
    "\n",
    "    # get the text of the plotted words\n",
    "    texts = []\n",
    "    for word, (x,y) in zip(words, low_dim_emb):\n",
    "        texts.append(plt.text(x+0.01, y+0.01, word, rotation=0))\n",
    "    \n",
    "    # adjust the position of the labels so that they dont overlap\n",
    "    adjust_text(texts)\n",
    "    # show plot\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "41a980f8-d2d7-4b7e-8aae-fb74ab132df7",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "41a980f8-d2d7-4b7e-8aae-fb74ab132df7",
    "outputId": "c78add96-08b6-49f3-ed39-0f50040f4391",
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "# define the tokens to use in the plot\n",
    "tokens_of_interest = ['economy', 'gdp', 'production', 'output',\n",
    "                      'investment', 'confidence', 'sentiment',\n",
    "                      'uncertainty', 'inflation', 'cpi',\n",
    "                      'loan', 'mortgage', 'credit', 'debt', 'savings', \n",
    "                      'borrowing', 'housing', 'labour', 'workforce', \n",
    "                      'unemployment', 'employment', 'jobs', 'wages',\n",
    "                      'trade', 'exports', 'imports']\n",
    "\n",
    "# expand the list of tokens with all the tokens from the replacement dictionary\n",
    "tokens_of_interest = set(tokens_of_interest + list(replacing_dict.values()) )\n",
    "\n",
    "# plot\n",
    "pca_scatterplot(word_vectors, list(tokens_of_interest))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3760eb2f",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "We can clearly observe how words form some thematically cohesive groups; trade (e.g. exports, imports, trade, output), job-market (e.g. workforce, jobs, employment), housing (e.g. real-state, housing, borrowing, mortgage)."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "af539380-686d-4ec5-924c-9f8bd51ace1e",
   "metadata": {
    "id": "af539380-686d-4ec5-924c-9f8bd51ace1e",
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "### *Nearest neighbors analysis*\n",
    "\n",
    "We can further explore how words cluster in the embedded space by analyzing the nearest neighbours of some selected words."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d5eadc3a-2711-4369-8c42-8df7c13af56a",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "d5eadc3a-2711-4369-8c42-8df7c13af56a",
    "outputId": "49c2d87d-c71d-4667-8415-8e600773d314",
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "# find the K nearest neighbours of relevant words\n",
    "K = 10\n",
    "words = [\"uncertainty\", \"risk\", \"stable\",\n",
    "         \"contraction\", \"expansion\",\n",
    "         \"monetary-policy\", \"interest-rate\", \"inflation\"]\n",
    "\n",
    "for word in words:\n",
    "    print(f\"Nearest neighbors of: {word}\")\n",
    "    print(word_vectors.most_similar(word, topn=K))\n",
    "    print(\"\\n\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "21cf8616-e4ae-4ca5-936d-eb9c0f0c6f03",
   "metadata": {
    "id": "21cf8616-e4ae-4ca5-936d-eb9c0f0c6f03",
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "### *Analogy tasks*"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6831515e",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "A very interesting, and surprising, use of word embeddings is to find word analogies. The famous example used by [Mikolov et al. (2013)](https://arxiv.org/pdf/1301.3781.pdf) searches for a word $X$ in the embedded space that is similar to \"woman\" in the same sense that \"king\" is similar to \"man\". This task can be expressed in terms of a simple vector arithmetic problem as follows:\n",
    "\n",
    "$$\n",
    "\\vec{King}^{\\,} - \\vec{Man}^{\\,} = \\vec{X}^{\\,} - \\vec{Woman}^{\\,} \\\\\n",
    "\\vec{King}^{\\,} - \\vec{Man}^{\\,} + \\vec{Woman}^{\\,} = \\vec{X}^{\\,}\n",
    "$$\n",
    "\n",
    "Mikolov et al. (2013) find that when performing this operation on their trained embeddings, they are able to recover the word \"queen\".\n",
    "\n",
    "$$ \\vec{King}^{\\,} - \\vec{Man}^{\\,} + \\vec{Woman}^{\\,} \\approx \\vec{Queen}^{\\,} $$\n",
    "\n",
    "Using ```Gensim``` this operation can be very easily perfomed by simply using the ```.most_similar()``` function as follows:\n",
    "\n",
    "<center>\n",
    "\n",
    "```python\n",
    "word_vectors.most_similar(positive=['woman', 'king'], negative=['man'])\n",
    "```\n",
    "\n",
    "</center>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2279c969",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "We will play with this idea and try to extend it to our own domain. Some of the analogies that we will try to solve are: \n",
    "\n",
    "$$\n",
    "\\vec{Contraction}^{\\,} - \\vec{Expansion}^{\\,} + \\vec{Downward}^{\\,} = \\vec{X}^{\\,} \\\\\n",
    "\\vec{Inflation}^{\\,} - \\vec{CPI}^{\\,} + \\vec{GDP}^{\\,} = \\vec{X}^{\\,} \\\\\n",
    "$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "PrOTPikwjPs1",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "PrOTPikwjPs1",
    "outputId": "b3fce250-d064-4060-d232-77f54da8ee1b",
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "# create the analogy tasks for our data\n",
    "positive_words = [['contraction', 'downward'],\n",
    "                  ['expansion', 'tighten'],\n",
    "                  ['inflation', 'gdp'],\n",
    "                  ['company', 'wages']]\n",
    "\n",
    "negative_words = [['expansion'],\n",
    "                  ['contraction'],\n",
    "                  ['cpi'],\n",
    "                  ['profits']]\n",
    "\n",
    "for pw, nw in zip(positive_words, negative_words):\n",
    "    print(f\"Analogy task for positive words: {pw} and negative words {nw}\")\n",
    "    print(word_vectors.most_similar(positive=pw, negative=nw))\n",
    "    print(\"\\n\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "aaca13c6-d38d-4b16-aa92-ab6623b4d47d",
   "metadata": {
    "id": "aaca13c6-d38d-4b16-aa92-ab6623b4d47d",
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "### *Building dictionaries*\n",
    "\n",
    "One last use of word embeddings is to expand existing dictionaries by finding the nearest neighbours to a set of \"center\" terms. To illustrate this, we will show how to generate dictionaries of positive and negative terms to analyze text data from the Bank of England Monetary Police Comittee minutes."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "be80cb35-d6e7-47a8-8b67-1ff6e2886357",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "be80cb35-d6e7-47a8-8b67-1ff6e2886357",
    "outputId": "3998ea5b-b894-4e0b-841d-b8633a5c99a0",
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "# create a positive dictionary by finding the nearest neighbors to a combination of relevant words\n",
    "N = 40\n",
    "pos_center_terms = ['expansion', 'stable']\n",
    "pos_nn = [w for w, _ in word_vectors.most_similar(positive=pos_center_terms, topn=N)]\n",
    "pos_word2vec = pos_center_terms + pos_nn\n",
    "print(pos_word2vec)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a03929de-be58-4d54-a1a5-a8af2b7d80be",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "a03929de-be58-4d54-a1a5-a8af2b7d80be",
    "outputId": "6454c3f3-96f7-46b4-fcc2-356ac97db855",
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "# create a negative dictionary by finding the nearest neighbors to a combination of relevant words\n",
    "N = 40\n",
    "neg_center_terms = ['contraction', 'uncertainty']\n",
    "neg_nn = [w for w, _ in word_vectors.most_similar(positive=neg_center_terms, topn=N)]\n",
    "neg_word2vec = neg_center_terms + neg_nn\n",
    "print(neg_word2vec)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1c41d896-29cf-4abe-8fb1-fdd5631497cc",
   "metadata": {
    "id": "1c41d896-29cf-4abe-8fb1-fdd5631497cc",
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "# load data for dictionary method example\n",
    "path_dict_example = data_path + 'mpc_minutes.txt'\n",
    "data_dict, prep_dict = dictionary_methods.dict_example(path_dict_example) # dataframe, preprocessing object"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d0f958ce-2d78-469f-abae-a3dc5c7e66c4",
   "metadata": {
    "id": "d0f958ce-2d78-469f-abae-a3dc5c7e66c4",
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "# generate the count of positive and negative lemmas in the corpus with our new dictionaries\n",
    "pos_counts_word2vec, neg_counts_word2vec = dictionary_methods.pos_neg_counts(prep_dict, pos_word2vec, neg_word2vec)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fb5e7e17-80cd-4d48-be7e-2d1e00d02d15",
   "metadata": {
    "id": "fb5e7e17-80cd-4d48-be7e-2d1e00d02d15",
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "# Apel and Blix-Grimaldi (2012) dictionaries\n",
    "pos_words_AB = ['accelerate','accelerated','accelerates','accelerating','expand','expanded','expanding','expands',\n",
    "             'fast','faster','fastest','gain','gained','gaining','gains','high','higher','highest','increase',\n",
    "             'increased','increases','increasing','strong','stronger','strongest']\n",
    "\n",
    "neg_words_AB = ['contract','contracted','contracting','contracts','decelerate','decelerated','decelerates',\n",
    "             'decelerating','decrease','decreased','decreases','decreasing','lose','losing','loss','losses',\n",
    "             'lost','low','lower','lowest','slow','slower','slowest','weak','weaker','weakest']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ad59b6fa-6d86-4409-9c80-77ff161cf8ca",
   "metadata": {
    "id": "ad59b6fa-6d86-4409-9c80-77ff161cf8ca",
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "# generate the count of positive and negative lemmas in the corpus with Apel and Blix-Grimaldi (2012)\n",
    "pos_counts_AB, neg_counts_AB = dictionary_methods.pos_neg_counts(prep_dict, pos_words_AB, neg_words_AB)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a2dd787a-ffc5-4a21-a163-8ac8d8629f90",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 203
    },
    "id": "a2dd787a-ffc5-4a21-a163-8ac8d8629f90",
    "outputId": "70577fdc-0daa-49b8-e3c9-868b78392ac6",
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "# add counts to the data\n",
    "data_dict['pos_counts_word2vec'] = pos_counts_word2vec\n",
    "data_dict['neg_counts_word2vec'] = neg_counts_word2vec\n",
    "\n",
    "data_dict['pos_counts_AB'] = pos_counts_AB\n",
    "data_dict['neg_counts_AB'] = neg_counts_AB\n",
    "\n",
    "data_dict.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "959c009e-b413-47f1-bd14-ea30d92f1a01",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 234
    },
    "id": "959c009e-b413-47f1-bd14-ea30d92f1a01",
    "outputId": "8f4f6871-bdba-4b41-ff1f-c0258c84c819",
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "# aggregate to year-month level\n",
    "data_agg = data_dict.groupby(['date']).agg({'pos_counts_word2vec': 'sum', 'neg_counts_word2vec': 'sum',\n",
    "                                            'pos_counts_AB': 'sum', 'neg_counts_AB': 'sum',\n",
    "                                            'year': 'mean', 'quarter':'mean'})\n",
    "data_agg.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "zgYZn0Jx3pNL",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 234
    },
    "id": "zgYZn0Jx3pNL",
    "outputId": "0ca199bb-0356-405f-f104-4aeb14a194f2",
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "# aggregate to year-quarter level removing incomplete quarters \n",
    "data_agg['months_x_quarter'] = 1\n",
    "data_agg = data_agg.groupby(['year', 'quarter']).sum()[['pos_counts_word2vec', 'neg_counts_word2vec',\n",
    "                                                        'pos_counts_AB', 'neg_counts_AB',\n",
    "                                                        'months_x_quarter']]\n",
    "\n",
    "data_agg = data_agg[data_agg['months_x_quarter']==3]\n",
    "del data_agg['months_x_quarter']\n",
    "\n",
    "data_agg.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b8e12c9b-3ea7-493b-8c8f-15677b66a2af",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 234
    },
    "id": "b8e12c9b-3ea7-493b-8c8f-15677b66a2af",
    "outputId": "18137c6e-0f84-4e90-8e87-365a557cad2d",
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "# compute sentiment at year-quarter level\n",
    "data_agg['sentiment_word2vec'] = (data_agg.pos_counts_word2vec - data_agg.neg_counts_word2vec)/(data_agg.pos_counts_word2vec + data_agg.neg_counts_word2vec)\n",
    "data_agg['sentiment_AB'] = (data_agg.pos_counts_AB - data_agg.neg_counts_AB)/(data_agg.pos_counts_AB + data_agg.neg_counts_AB)\n",
    "data_agg.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1a146d7a-9377-4846-9397-dafd0d837c77",
   "metadata": {
    "id": "1a146d7a-9377-4846-9397-dafd0d837c77",
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "Next we add quarterly GDP data collected from the ONS website."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a839e6bc-9e20-4cfc-92af-4ef322758f09",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 203
    },
    "id": "a839e6bc-9e20-4cfc-92af-4ef322758f09",
    "outputId": "8d70c8fc-d366-472a-9111-6a004633d989",
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "# prepare GDP data\n",
    "ons = pd.read_csv(data_path + 'ons_quarterly_gdp.csv', names=['label', 'gdp_growth', 'quarter_long'], header=0)\n",
    "ons['year'] = ons.label.apply(lambda x: x[:4]).astype(int)\n",
    "ons['quarter'] = ons.label.apply(lambda x: x[6]).astype(int)\n",
    "ons = ons[['year', 'quarter', 'gdp_growth']]\n",
    "ons = ons.drop_duplicates().reset_index(drop=True).copy()\n",
    "ons.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8a63d1e5-0951-4899-bbb6-215c23f670a5",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 203
    },
    "id": "8a63d1e5-0951-4899-bbb6-215c23f670a5",
    "outputId": "c0ac367b-8a39-4b8f-ebb5-1e292ba2a3cd",
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "# merge to sentiment data\n",
    "df = data_agg.merge(ons, how='left', on=['year', 'quarter']).copy()\n",
    "# create year-quarter variable\n",
    "df[\"year_quarter\"] = df.apply(lambda x: f\"{int(x['quarter'])}Q{int(x['year'])}\", axis=1)\n",
    "df[\"year_quarter\"] = df[\"year_quarter\"].apply(lambda x: pd.Period(value=x, freq=\"Q\").to_timestamp())\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "577dd00e-a66f-48e1-aba8-4d4ded3d5f7e",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "577dd00e-a66f-48e1-aba8-4d4ded3d5f7e",
    "outputId": "e9e43a7d-c295-4e4b-b1ed-06ba79c88523",
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "print(df[['sentiment_AB', 'sentiment_word2vec', 'gdp_growth']].corr())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ffbd0886-fc52-46f0-b51f-1b3dcf52a014",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 483
    },
    "id": "ffbd0886-fc52-46f0-b51f-1b3dcf52a014",
    "outputId": "ca9c9839-84f5-489a-8fd3-bd61cd1856d4",
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "scaler = StandardScaler()\n",
    "\n",
    "fig, ax = plt.subplots(figsize=(16,8))\n",
    "ax.plot(df[\"year_quarter\"], scaler.fit_transform(df.sentiment_AB.values.reshape(-1, 1)), label=\"Apel and Blix-Grimaldi (2012)\")\n",
    "ax.plot(df[\"year_quarter\"], scaler.fit_transform(df.sentiment_word2vec.values.reshape(-1, 1)), label=\"Word2Vec dictionaries\")\n",
    "ax.plot(df[\"year_quarter\"], scaler.fit_transform(df.gdp_growth.values.reshape(-1, 1)), label=\"GDP Growth\")\n",
    "\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "collapsed_sections": [
    "729f5f5e-6a29-4d33-9d23-115f499420a4"
   ],
   "name": "word2vec_notebook_private.ipynb",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}